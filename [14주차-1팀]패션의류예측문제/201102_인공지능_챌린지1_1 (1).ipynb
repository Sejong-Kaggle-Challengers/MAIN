{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "201102 인공지능 챌린지1-1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKLKFbvQPZ0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b12080ca-d68d-474f-8728-0109df6b3b4f"
      },
      "source": [
        "!pip install kaggle\n",
        "!pip install --upgrade pip\n",
        "!pip install kaggle==1.5.6\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.6)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2020.12.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.0.1)\n",
            "Requirement already satisfied: kaggle==1.5.6 in /usr/local/lib/python3.7/dist-packages (1.5.6)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle==1.5.6) (4.0.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle==1.5.6) (2.8.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle==1.5.6) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle==1.5.6) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle==1.5.6) (4.41.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from kaggle==1.5.6) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle==1.5.6) (2020.12.5)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle==1.5.6) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle==1.5.6) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle==1.5.6) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzwG5jvVThss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "032e72c4-a1b8-42dc-94b5-cfee47aefad0"
      },
      "source": [
        "!kaggle competitions download -c sejong-ai-challenge-p1"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading sejong-ai-challenge-p1.zip to /content\n",
            "\r  0% 0.00/23.8M [00:00<?, ?B/s]\r 21% 5.00M/23.8M [00:00<00:00, 34.3MB/s]\r 76% 18.0M/23.8M [00:00<00:00, 44.1MB/s]\n",
            "\r100% 23.8M/23.8M [00:00<00:00, 92.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EctNqoIBT3-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69741eaf-5944-414f-da7e-7bb5564211e2"
      },
      "source": [
        "!unzip sejong-ai-challenge-p1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  sejong-ai-challenge-p1.zip\n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unximdpFF3XD"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import torch\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import cv2\n",
        "import os\n",
        "from random import *\n",
        "import time\n",
        "import copy"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFJlhP40T7lm"
      },
      "source": [
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "\n",
        "train = pd.read_csv('train.csv')\n",
        "for i in range(0,33839):\n",
        "  if (train['label'][i] == \"missing\" or train['label'][i] == \"NAN\"):\n",
        "    train= train.drop([i])\n",
        "test = pd.read_csv('test.csv')\n",
        "submit = pd.read_csv('sample_submission.csv')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "kdCTyeWMtBma",
        "outputId": "edb59d39-9532-482e-efb0-61d19e36718e"
      },
      "source": [
        "train"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>pixel10</th>\n",
              "      <th>pixel11</th>\n",
              "      <th>pixel12</th>\n",
              "      <th>pixel13</th>\n",
              "      <th>pixel14</th>\n",
              "      <th>pixel15</th>\n",
              "      <th>pixel16</th>\n",
              "      <th>pixel17</th>\n",
              "      <th>pixel18</th>\n",
              "      <th>pixel19</th>\n",
              "      <th>pixel20</th>\n",
              "      <th>pixel21</th>\n",
              "      <th>pixel22</th>\n",
              "      <th>pixel23</th>\n",
              "      <th>pixel24</th>\n",
              "      <th>pixel25</th>\n",
              "      <th>pixel26</th>\n",
              "      <th>pixel27</th>\n",
              "      <th>pixel28</th>\n",
              "      <th>pixel29</th>\n",
              "      <th>pixel30</th>\n",
              "      <th>pixel31</th>\n",
              "      <th>pixel32</th>\n",
              "      <th>pixel33</th>\n",
              "      <th>pixel34</th>\n",
              "      <th>pixel35</th>\n",
              "      <th>pixel36</th>\n",
              "      <th>pixel37</th>\n",
              "      <th>pixel38</th>\n",
              "      <th>pixel39</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel746</th>\n",
              "      <th>pixel747</th>\n",
              "      <th>pixel748</th>\n",
              "      <th>pixel749</th>\n",
              "      <th>pixel750</th>\n",
              "      <th>pixel751</th>\n",
              "      <th>pixel752</th>\n",
              "      <th>pixel753</th>\n",
              "      <th>pixel754</th>\n",
              "      <th>pixel755</th>\n",
              "      <th>pixel756</th>\n",
              "      <th>pixel757</th>\n",
              "      <th>pixel758</th>\n",
              "      <th>pixel759</th>\n",
              "      <th>pixel760</th>\n",
              "      <th>pixel761</th>\n",
              "      <th>pixel762</th>\n",
              "      <th>pixel763</th>\n",
              "      <th>pixel764</th>\n",
              "      <th>pixel765</th>\n",
              "      <th>pixel766</th>\n",
              "      <th>pixel767</th>\n",
              "      <th>pixel768</th>\n",
              "      <th>pixel769</th>\n",
              "      <th>pixel770</th>\n",
              "      <th>pixel771</th>\n",
              "      <th>pixel772</th>\n",
              "      <th>pixel773</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "      <th>pixel784</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>105</td>\n",
              "      <td>92</td>\n",
              "      <td>101</td>\n",
              "      <td>107</td>\n",
              "      <td>100</td>\n",
              "      <td>132</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>...</td>\n",
              "      <td>220</td>\n",
              "      <td>214</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "      <td>255</td>\n",
              "      <td>222</td>\n",
              "      <td>128</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>134</td>\n",
              "      <td>162</td>\n",
              "      <td>191</td>\n",
              "      <td>214</td>\n",
              "      <td>163</td>\n",
              "      <td>146</td>\n",
              "      <td>165</td>\n",
              "      <td>79</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>46</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>187</td>\n",
              "      <td>189</td>\n",
              "      <td>...</td>\n",
              "      <td>237</td>\n",
              "      <td>229</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>68</td>\n",
              "      <td>116</td>\n",
              "      <td>112</td>\n",
              "      <td>136</td>\n",
              "      <td>147</td>\n",
              "      <td>144</td>\n",
              "      <td>121</td>\n",
              "      <td>102</td>\n",
              "      <td>63</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>159</td>\n",
              "      <td>161</td>\n",
              "      <td>143</td>\n",
              "      <td>180</td>\n",
              "      <td>142</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>231</td>\n",
              "      <td>241</td>\n",
              "      <td>217</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>36</td>\n",
              "      <td>50</td>\n",
              "      <td>51</td>\n",
              "      <td>68</td>\n",
              "      <td>48</td>\n",
              "      <td>48</td>\n",
              "      <td>33</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33834</th>\n",
              "      <td>59989</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>89</td>\n",
              "      <td>62</td>\n",
              "      <td>82</td>\n",
              "      <td>29</td>\n",
              "      <td>3</td>\n",
              "      <td>39</td>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>164</td>\n",
              "      <td>238</td>\n",
              "      <td>249</td>\n",
              "      <td>...</td>\n",
              "      <td>228</td>\n",
              "      <td>210</td>\n",
              "      <td>233</td>\n",
              "      <td>117</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>132</td>\n",
              "      <td>133</td>\n",
              "      <td>171</td>\n",
              "      <td>198</td>\n",
              "      <td>217</td>\n",
              "      <td>209</td>\n",
              "      <td>210</td>\n",
              "      <td>198</td>\n",
              "      <td>168</td>\n",
              "      <td>144</td>\n",
              "      <td>122</td>\n",
              "      <td>131</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33835</th>\n",
              "      <td>59990</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>221</td>\n",
              "      <td>214</td>\n",
              "      <td>203</td>\n",
              "      <td>202</td>\n",
              "      <td>200</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>242</td>\n",
              "      <td>229</td>\n",
              "      <td>223</td>\n",
              "      <td>240</td>\n",
              "      <td>104</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>184</td>\n",
              "      <td>181</td>\n",
              "      <td>170</td>\n",
              "      <td>173</td>\n",
              "      <td>170</td>\n",
              "      <td>169</td>\n",
              "      <td>171</td>\n",
              "      <td>170</td>\n",
              "      <td>161</td>\n",
              "      <td>158</td>\n",
              "      <td>154</td>\n",
              "      <td>161</td>\n",
              "      <td>165</td>\n",
              "      <td>83</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33836</th>\n",
              "      <td>59991</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33837</th>\n",
              "      <td>59992</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33839</th>\n",
              "      <td>59999</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>28953 rows × 786 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  pixel1  pixel2  pixel3  ...  pixel782  pixel783  pixel784  label\n",
              "1          1       0       0       0  ...         0         0         0      9\n",
              "2          2       0       0       0  ...         0         0         0      6\n",
              "4          4       0       0       0  ...         0         0         0      3\n",
              "5          5       0       0       0  ...         0         0         0      4\n",
              "6          6       0       0       0  ...         0         0         0      4\n",
              "...      ...     ...     ...     ...  ...       ...       ...       ...    ...\n",
              "33834  59989       0       0       0  ...         0         0         0      4\n",
              "33835  59990       0       0       0  ...         0         0         0      0\n",
              "33836  59991       0       0       0  ...         0         0         0      5\n",
              "33837  59992       0       0       0  ...         0         0         0      5\n",
              "33839  59999       0       0       0  ...         0         0         0      7\n",
              "\n",
              "[28953 rows x 786 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQNO-CUyUJAS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bfb5d36-e13b-47aa-bbd0-b8614827f953"
      },
      "source": [
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28953, 786)\n",
            "(8460, 785)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "296COXRenqZg",
        "outputId": "1b7d7083-8627-4801-f84d-38f94f5469d7"
      },
      "source": [
        "train['label'].drop_duplicates()\n",
        "#라벨 2가 없음\n",
        "#이 상태로 학습하면 예측값에 2가 안 나오나?"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1      9\n",
              "2      6\n",
              "4      3\n",
              "5      4\n",
              "7      5\n",
              "9      8\n",
              "22     7\n",
              "23     1\n",
              "755    0\n",
              "Name: label, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUX-pHGZ_g_c"
      },
      "source": [
        "train = train.drop(['id'], 1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "ycbg2-DIAE5O",
        "outputId": "bb00a910-37ff-477a-c4eb-24c37607da8d"
      },
      "source": [
        "x_image_train = train.iloc[:, 0:784].to_numpy()\n",
        "\n",
        "x_image_train = x_image_train.reshape(-1, 28,28).astype(int)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = x_image_train[6]\n",
        "plt.imshow(img, cmap='gray')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fcb1f3fbc10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdfBTkMKUP7U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be5de36d-1383-47cc-cac9-548af3a660d5"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train = train.iloc[:, 0:-1].to_numpy()\n",
        "\n",
        "y_train = train.iloc[:,-1]\n",
        "y_train = np.array(y_train)\n",
        "#y_train = torch.FloatTensor(y_train)\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.3, random_state=1, stratify = y_train)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_val.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20267, 784)\n",
            "(8686, 784)\n",
            "(20267,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUExmS9zlTgQ"
      },
      "source": [
        "def make_folder(directory_path):\n",
        "    if not os.path.isdir(directory_path):\n",
        "        os.mkdir(directory_path)\n",
        "\n",
        "path_train = os.path.join(os.getcwd(), 'train')\n",
        "path_val = os.path.join(os.getcwd(), 'val')\n",
        "path_test = os.path.join(os.getcwd(), 'test')\n",
        "\n",
        "make_folder(path_train)\n",
        "make_folder(path_val)\n",
        "make_folder(path_test)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-cwvJd5qNzA"
      },
      "source": [
        "for i in (0,1,3,4,5,6,7,8,9):\n",
        "    path_train_digit = os.path.join(path_train, str(i))\n",
        "    path_val_digit = os.path.join(path_val, str(i))\n",
        "    make_folder(path_train_digit)\n",
        "    make_folder(path_val_digit)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q_mf8caqw1R"
      },
      "source": [
        "for i in range(len(x_train)):\n",
        "    label = y_train[i]\n",
        "    img = x_train[i].reshape(28,28).astype(int)\n",
        "    path_train_digit = os.path.join(path_train, str(label))\n",
        "\n",
        "    if label == '0':\n",
        "        path_train_image = os.path.join(path_train_digit, '%d.jpg'%(i))\n",
        "        cv2.imwrite(path_train_image, img)\n",
        "\n",
        "    elif label == '1':\n",
        "        path_train_image = os.path.join(path_train_digit, '%d.jpg'%(i))\n",
        "        cv2.imwrite(path_train_image, img)\n",
        "    \n",
        "    elif label == '3':\n",
        "        path_train_image = os.path.join(path_train_digit, '%d.jpg'%(i))\n",
        "        cv2.imwrite(path_train_image, img)\n",
        "\n",
        "    elif label == '4':\n",
        "        path_train_image = os.path.join(path_train_digit, '%d.jpg'%(i))\n",
        "        cv2.imwrite(path_train_image, img)\n",
        "    \n",
        "    elif label == '5':\n",
        "        path_train_image = os.path.join(path_train_digit, '%d.jpg'%(i))\n",
        "        cv2.imwrite(path_train_image, img)\n",
        "\n",
        "    elif label == '6':\n",
        "        path_train_image = os.path.join(path_train_digit, '%d.jpg'%(i))\n",
        "        cv2.imwrite(path_train_image, img)\n",
        "\n",
        "    elif label == '7':\n",
        "        path_train_image = os.path.join(path_train_digit, '%d.jpg'%(i))\n",
        "        cv2.imwrite(path_train_image, img)\n",
        "    \n",
        "    elif label == '8':\n",
        "        path_train_image = os.path.join(path_train_digit, '%d.jpg'%(i))\n",
        "        cv2.imwrite(path_train_image, img)\n",
        "    \n",
        "    elif label == '9':\n",
        "        path_train_image = os.path.join(path_train_digit, '%d.jpg'%(i))\n",
        "        cv2.imwrite(path_train_image, img)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgPJDBZn59Wv"
      },
      "source": [
        "for i in range(len(x_val)):\n",
        "    label = y_val[i]\n",
        "    img = x_val[i].reshape(28,28).astype(int)\n",
        "    path_val_digit = os.path.join(path_val, str(label))\n",
        "\n",
        "    if label == '0':\n",
        "        path_val_image = os.path.join(path_val_digit, '%d.jpg'%(i))\n",
        "        cv2.imwrite(path_val_image, img)\n",
        "\n",
        "    elif label == '1':\n",
        "        path_val_image = os.path.join(path_val_digit, '%d.jpg'%(i))\n",
        "        cv2.imwrite(path_val_image, img)\n",
        "    \n",
        "    elif label == '3':\n",
        "        path_val_image = os.path.join(path_val_digit, '%d.jpg'%(i))\n",
        "        cv2.imwrite(path_val_image, img)\n",
        "\n",
        "    elif label == '4':\n",
        "        path_val_image = os.path.join(path_val_digit, '%d.jpg'%(i))\n",
        "        cv2.imwrite(path_val_image, img)\n",
        "    \n",
        "    elif label == '5':\n",
        "        path_val_image = os.path.join(path_val_digit, '%d.jpg'%(i))\n",
        "        cv2.imwrite(path_val_image, img)\n",
        "\n",
        "    elif label == '6':\n",
        "        path_val_image = os.path.join(path_val_digit, '%d.jpg'%(i))\n",
        "        cv2.imwrite(path_val_image, img)\n",
        "\n",
        "    elif label == '7':\n",
        "        path_val_image = os.path.join(path_val_digit, '%d.jpg'%(i))\n",
        "        cv2.imwrite(path_val_image, img)\n",
        "    \n",
        "    elif label == '8':\n",
        "        path_val_image = os.path.join(path_val_digit, '%d.jpg'%(i))\n",
        "        cv2.imwrite(path_val_image, img)\n",
        "    \n",
        "    elif label == '9':\n",
        "        path_val_image = os.path.join(path_val_digit, '%d.jpg'%(i))\n",
        "        cv2.imwrite(path_val_image, img)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzHmV9yd788B"
      },
      "source": [
        "test = test.drop(['id'], 1)\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_37FJxYB7ghy"
      },
      "source": [
        "for i in range(len(test)):\n",
        "    img = test.iloc[i, 0:].to_numpy().reshape(28,28).astype(int)\n",
        "    path_test_digit = os.path.join(path_test, '%d.jpg'%(i))\n",
        "    cv2.imwrite(path_test_digit, img)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDfoj3RA-ehV"
      },
      "source": [
        "img_size = 64\n",
        "data_transforms = {\n",
        "    'train' : transforms.Compose([\n",
        "        transforms.Resize([img_size, img_size]),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(20),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor(), #데이터 타입을 tensor로 변경\n",
        "        transforms.Normalize([0.5],[0.5])\n",
        "        #이미지의 경우 픽셀 값이 0~255의 값을 가지는데, ToTensor로 0~1사이의 값으로 바뀜\n",
        "        #normalize시 -1~1사이의 값으로 normalize 됨\n",
        "\n",
        "        # ToTensor ==> scaling\n",
        "        # Normalize ==> centerizing+rescaling\n",
        "    ]),\n",
        "    'val':transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.Resize([img_size, img_size]),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5],[0.5])\n",
        "    ])\n",
        "}"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFRUtttQ-jO9",
        "outputId": "a8344be8-1012-482b-ebfc-2210d1f59382"
      },
      "source": [
        "data_dir = os.path.join(os.getcwd())\n",
        "\n",
        "image_datasets = {x:datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                         data_transforms[x]) for x in ['train', 'val']}\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=20, shuffle=True, num_workers=20) for x in ['train', 'val']}\n",
        "dataset_sizes = {x:len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "print(dataset_sizes, class_names)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'train': 20267, 'val': 8686} ['0', '1', '3', '4', '5', '6', '7', '8', '9']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8LzylIv-r36",
        "outputId": "94effd02-2420-45bb-ebe6-64eb9488fd91"
      },
      "source": [
        "#GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czV_n28CFWeS"
      },
      "source": [
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomCNN, self).__init__()\n",
        "\n",
        "        self.layer1 = self.conv_module(1, 16)\n",
        "        self.layer3 = self.conv_module(16, 32)\n",
        "        self.layer4 = self.conv_module(32, 64)\n",
        "        self.layer5 = self.conv_module(64, 128)\n",
        "        self.layer6 = self.conv_module(128,256)\n",
        "        self.gap = self.global_avg_pool(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = self.layer6(out)\n",
        "        out = self.gap(out)\n",
        "        out = out.view(-1, 10)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def conv_module(self, in_num, out_num):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_num, out_num, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_num),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1))\n",
        "\n",
        "    def global_avg_pool(self, in_num, out_num):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_num, out_num, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(out_num),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxQN2porFYLw",
        "outputId": "cd8b710a-96b7-48e7-d4f7-1add52b2cf05"
      },
      "source": [
        "model = CustomCNN()\n",
        "model = model.to(device)\n",
        "print(model)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CustomCNN(\n",
            "  (layer1): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.01)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.01)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.01)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (layer5): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.01)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (layer6): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.01)\n",
            "    (3): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (gap): Sequential(\n",
            "    (0): Conv2d(256, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.01)\n",
            "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26qnZidgFaA5"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
        "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor=0.1,patience=5)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rX6KXIGFbii"
      },
      "source": [
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=8):\n",
        "    global_info=[]\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict()) #deepcopy 내부에 객체들까지 새롭게 copy 되는 것\n",
        "    best_acc = 0.0\n",
        "    early_stopping = EarlyStopping(patience=11, verbose=True)\n",
        "    for epoch in range(num_epochs):\n",
        "        local_info = []\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "                \n",
        "                if epoch >0:\n",
        "                    scheduler.step(val_loss)\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                \n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            if phase == 'val':\n",
        "                val_loss = running_loss / dataset_sizes['val']\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            #(Variable(x).data).cpu().numpy()\n",
        "            if phase == 'train':\n",
        "                local_info.append(epoch_loss)\n",
        "                ea = epoch_acc.cpu().numpy()\n",
        "                local_info.append(ea)\n",
        "            else:\n",
        "                local_info.append(epoch_loss)\n",
        "                ea = epoch_acc.cpu().numpy()\n",
        "                local_info.append(ea)\n",
        "\n",
        "\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        lr_get = get_lr(optimizer)\n",
        "        print(\"Current learning rate : {:.8f}\".format(lr_get))\n",
        "        global_info.append(local_info)\n",
        "        if phase =='val':\n",
        "            early_stopping(epoch_loss, model)\n",
        "\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "    \n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlsoDafM-26t",
        "outputId": "d86094e0-e390-4a47-9dde-2697a6ca478d"
      },
      "source": [
        "model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=100)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n",
            "train Loss: 1.0795 Acc: 0.7172\n",
            "val Loss: 0.8609 Acc: 0.7065\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (inf --> 0.860889).  Saving model ...\n",
            "Epoch 1/99\n",
            "----------\n",
            "train Loss: 0.6513 Acc: 0.7925\n",
            "val Loss: 0.5532 Acc: 0.8093\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (0.860889 --> 0.553171).  Saving model ...\n",
            "Epoch 2/99\n",
            "----------\n",
            "train Loss: 0.5599 Acc: 0.8139\n",
            "val Loss: 0.5548 Acc: 0.7785\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 3/99\n",
            "----------\n",
            "train Loss: 0.5096 Acc: 0.8289\n",
            "val Loss: 0.4798 Acc: 0.8283\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (0.553171 --> 0.479820).  Saving model ...\n",
            "Epoch 4/99\n",
            "----------\n",
            "train Loss: 0.4737 Acc: 0.8377\n",
            "val Loss: 0.4944 Acc: 0.8074\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 5/99\n",
            "----------\n",
            "train Loss: 0.4523 Acc: 0.8430\n",
            "val Loss: 0.4365 Acc: 0.8443\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (0.479820 --> 0.436514).  Saving model ...\n",
            "Epoch 6/99\n",
            "----------\n",
            "train Loss: 0.4334 Acc: 0.8500\n",
            "val Loss: 0.3603 Acc: 0.8739\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (0.436514 --> 0.360257).  Saving model ...\n",
            "Epoch 7/99\n",
            "----------\n",
            "train Loss: 0.4220 Acc: 0.8538\n",
            "val Loss: 0.3689 Acc: 0.8670\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 8/99\n",
            "----------\n",
            "train Loss: 0.4064 Acc: 0.8577\n",
            "val Loss: 0.4062 Acc: 0.8460\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 9/99\n",
            "----------\n",
            "train Loss: 0.3923 Acc: 0.8644\n",
            "val Loss: 0.3340 Acc: 0.8815\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (0.360257 --> 0.333979).  Saving model ...\n",
            "Epoch 10/99\n",
            "----------\n",
            "train Loss: 0.3861 Acc: 0.8644\n",
            "val Loss: 0.3456 Acc: 0.8790\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 11/99\n",
            "----------\n",
            "train Loss: 0.3727 Acc: 0.8686\n",
            "val Loss: 0.3691 Acc: 0.8562\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 12/99\n",
            "----------\n",
            "train Loss: 0.3637 Acc: 0.8734\n",
            "val Loss: 0.3423 Acc: 0.8752\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 13/99\n",
            "----------\n",
            "train Loss: 0.3537 Acc: 0.8769\n",
            "val Loss: 0.3103 Acc: 0.8869\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (0.333979 --> 0.310311).  Saving model ...\n",
            "Epoch 14/99\n",
            "----------\n",
            "train Loss: 0.3442 Acc: 0.8795\n",
            "val Loss: 0.3025 Acc: 0.8878\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (0.310311 --> 0.302502).  Saving model ...\n",
            "Epoch 15/99\n",
            "----------\n",
            "train Loss: 0.3431 Acc: 0.8809\n",
            "val Loss: 0.2925 Acc: 0.8915\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (0.302502 --> 0.292545).  Saving model ...\n",
            "Epoch 16/99\n",
            "----------\n",
            "train Loss: 0.3341 Acc: 0.8839\n",
            "val Loss: 0.2836 Acc: 0.9025\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (0.292545 --> 0.283587).  Saving model ...\n",
            "Epoch 17/99\n",
            "----------\n",
            "train Loss: 0.3282 Acc: 0.8848\n",
            "val Loss: 0.2883 Acc: 0.8980\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 18/99\n",
            "----------\n",
            "train Loss: 0.3241 Acc: 0.8853\n",
            "val Loss: 0.3085 Acc: 0.8880\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 19/99\n",
            "----------\n",
            "train Loss: 0.3193 Acc: 0.8880\n",
            "val Loss: 0.2709 Acc: 0.9026\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (0.283587 --> 0.270903).  Saving model ...\n",
            "Epoch 20/99\n",
            "----------\n",
            "train Loss: 0.3094 Acc: 0.8898\n",
            "val Loss: 0.3060 Acc: 0.8860\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 21/99\n",
            "----------\n",
            "train Loss: 0.3088 Acc: 0.8905\n",
            "val Loss: 0.2697 Acc: 0.8997\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (0.270903 --> 0.269706).  Saving model ...\n",
            "Epoch 22/99\n",
            "----------\n",
            "train Loss: 0.3036 Acc: 0.8909\n",
            "val Loss: 0.2949 Acc: 0.8850\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 23/99\n",
            "----------\n",
            "train Loss: 0.2969 Acc: 0.8927\n",
            "val Loss: 0.2617 Acc: 0.9051\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (0.269706 --> 0.261673).  Saving model ...\n",
            "Epoch 24/99\n",
            "----------\n",
            "train Loss: 0.2913 Acc: 0.8966\n",
            "val Loss: 0.2534 Acc: 0.9080\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (0.261673 --> 0.253364).  Saving model ...\n",
            "Epoch 25/99\n",
            "----------\n",
            "train Loss: 0.2903 Acc: 0.8954\n",
            "val Loss: 0.2975 Acc: 0.8911\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 26/99\n",
            "----------\n",
            "train Loss: 0.2870 Acc: 0.8966\n",
            "val Loss: 0.2637 Acc: 0.9010\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 27/99\n",
            "----------\n",
            "train Loss: 0.2835 Acc: 0.8995\n",
            "val Loss: 0.2829 Acc: 0.8941\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 28/99\n",
            "----------\n",
            "train Loss: 0.2779 Acc: 0.8989\n",
            "val Loss: 0.2539 Acc: 0.9046\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 4 out of 11\n",
            "Epoch 29/99\n",
            "----------\n",
            "train Loss: 0.2740 Acc: 0.9011\n",
            "val Loss: 0.2650 Acc: 0.9071\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 5 out of 11\n",
            "Epoch 30/99\n",
            "----------\n",
            "train Loss: 0.2703 Acc: 0.9029\n",
            "val Loss: 0.2340 Acc: 0.9123\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (0.253364 --> 0.234016).  Saving model ...\n",
            "Epoch 31/99\n",
            "----------\n",
            "train Loss: 0.2689 Acc: 0.9059\n",
            "val Loss: 0.2326 Acc: 0.9188\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (0.234016 --> 0.232622).  Saving model ...\n",
            "Epoch 32/99\n",
            "----------\n",
            "train Loss: 0.2662 Acc: 0.9048\n",
            "val Loss: 0.2407 Acc: 0.9104\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 33/99\n",
            "----------\n",
            "train Loss: 0.2632 Acc: 0.9053\n",
            "val Loss: 0.2280 Acc: 0.9163\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (0.232622 --> 0.227989).  Saving model ...\n",
            "Epoch 34/99\n",
            "----------\n",
            "train Loss: 0.2596 Acc: 0.9085\n",
            "val Loss: 0.2322 Acc: 0.9191\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 35/99\n",
            "----------\n",
            "train Loss: 0.2616 Acc: 0.9046\n",
            "val Loss: 0.2261 Acc: 0.9187\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (0.227989 --> 0.226065).  Saving model ...\n",
            "Epoch 36/99\n",
            "----------\n",
            "train Loss: 0.2528 Acc: 0.9098\n",
            "val Loss: 0.2324 Acc: 0.9147\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 37/99\n",
            "----------\n",
            "train Loss: 0.2520 Acc: 0.9102\n",
            "val Loss: 0.2420 Acc: 0.9085\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 38/99\n",
            "----------\n",
            "train Loss: 0.2527 Acc: 0.9101\n",
            "val Loss: 0.2346 Acc: 0.9153\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 39/99\n",
            "----------\n",
            "train Loss: 0.2483 Acc: 0.9097\n",
            "val Loss: 0.2386 Acc: 0.9141\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 4 out of 11\n",
            "Epoch 40/99\n",
            "----------\n",
            "train Loss: 0.2452 Acc: 0.9121\n",
            "val Loss: 0.2261 Acc: 0.9199\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 5 out of 11\n",
            "Epoch 41/99\n",
            "----------\n",
            "train Loss: 0.2411 Acc: 0.9133\n",
            "val Loss: 0.2210 Acc: 0.9196\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (0.226065 --> 0.220978).  Saving model ...\n",
            "Epoch 42/99\n",
            "----------\n",
            "train Loss: 0.2449 Acc: 0.9106\n",
            "val Loss: 0.2339 Acc: 0.9122\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 43/99\n",
            "----------\n",
            "train Loss: 0.2398 Acc: 0.9136\n",
            "val Loss: 0.2240 Acc: 0.9176\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 44/99\n",
            "----------\n",
            "train Loss: 0.2333 Acc: 0.9177\n",
            "val Loss: 0.2335 Acc: 0.9103\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 45/99\n",
            "----------\n",
            "train Loss: 0.2352 Acc: 0.9143\n",
            "val Loss: 0.2448 Acc: 0.9094\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 4 out of 11\n",
            "Epoch 46/99\n",
            "----------\n",
            "train Loss: 0.2364 Acc: 0.9144\n",
            "val Loss: 0.2164 Acc: 0.9208\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (0.220978 --> 0.216354).  Saving model ...\n",
            "Epoch 47/99\n",
            "----------\n",
            "train Loss: 0.2303 Acc: 0.9160\n",
            "val Loss: 0.2234 Acc: 0.9169\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 48/99\n",
            "----------\n",
            "train Loss: 0.2281 Acc: 0.9165\n",
            "val Loss: 0.2517 Acc: 0.9104\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 49/99\n",
            "----------\n",
            "train Loss: 0.2234 Acc: 0.9204\n",
            "val Loss: 0.2259 Acc: 0.9190\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 50/99\n",
            "----------\n",
            "train Loss: 0.2277 Acc: 0.9181\n",
            "val Loss: 0.2129 Acc: 0.9186\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (0.216354 --> 0.212857).  Saving model ...\n",
            "Epoch 51/99\n",
            "----------\n",
            "train Loss: 0.2248 Acc: 0.9188\n",
            "val Loss: 0.2083 Acc: 0.9249\n",
            "Current learning rate : 0.00300000\n",
            "Validation loss decreased (0.212857 --> 0.208270).  Saving model ...\n",
            "Epoch 52/99\n",
            "----------\n",
            "train Loss: 0.2211 Acc: 0.9211\n",
            "val Loss: 0.2262 Acc: 0.9186\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 53/99\n",
            "----------\n",
            "train Loss: 0.2210 Acc: 0.9214\n",
            "val Loss: 0.2246 Acc: 0.9142\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 54/99\n",
            "----------\n",
            "train Loss: 0.2181 Acc: 0.9202\n",
            "val Loss: 0.2176 Acc: 0.9204\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 55/99\n",
            "----------\n",
            "train Loss: 0.2171 Acc: 0.9206\n",
            "val Loss: 0.2381 Acc: 0.9115\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 4 out of 11\n",
            "Epoch 56/99\n",
            "----------\n",
            "train Loss: 0.2188 Acc: 0.9209\n",
            "val Loss: 0.2309 Acc: 0.9162\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 5 out of 11\n",
            "Epoch 57/99\n",
            "----------\n",
            "train Loss: 0.2137 Acc: 0.9253\n",
            "val Loss: 0.2130 Acc: 0.9183\n",
            "Current learning rate : 0.00300000\n",
            "EarlyStopping counter: 6 out of 11\n",
            "Epoch 58/99\n",
            "----------\n",
            "train Loss: 0.2118 Acc: 0.9242\n",
            "val Loss: 0.2111 Acc: 0.9238\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 7 out of 11\n",
            "Epoch 59/99\n",
            "----------\n",
            "train Loss: 0.1850 Acc: 0.9360\n",
            "val Loss: 0.1833 Acc: 0.9335\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (0.208270 --> 0.183299).  Saving model ...\n",
            "Epoch 60/99\n",
            "----------\n",
            "train Loss: 0.1794 Acc: 0.9365\n",
            "val Loss: 0.1844 Acc: 0.9333\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 61/99\n",
            "----------\n",
            "train Loss: 0.1760 Acc: 0.9394\n",
            "val Loss: 0.1843 Acc: 0.9330\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 62/99\n",
            "----------\n",
            "train Loss: 0.1780 Acc: 0.9386\n",
            "val Loss: 0.1831 Acc: 0.9339\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (0.183299 --> 0.183100).  Saving model ...\n",
            "Epoch 63/99\n",
            "----------\n",
            "train Loss: 0.1719 Acc: 0.9401\n",
            "val Loss: 0.1887 Acc: 0.9292\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 64/99\n",
            "----------\n",
            "train Loss: 0.1715 Acc: 0.9403\n",
            "val Loss: 0.1812 Acc: 0.9345\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (0.183100 --> 0.181203).  Saving model ...\n",
            "Epoch 65/99\n",
            "----------\n",
            "train Loss: 0.1707 Acc: 0.9396\n",
            "val Loss: 0.1839 Acc: 0.9322\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 66/99\n",
            "----------\n",
            "train Loss: 0.1682 Acc: 0.9423\n",
            "val Loss: 0.1815 Acc: 0.9347\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 67/99\n",
            "----------\n",
            "train Loss: 0.1686 Acc: 0.9428\n",
            "val Loss: 0.1814 Acc: 0.9353\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 68/99\n",
            "----------\n",
            "train Loss: 0.1708 Acc: 0.9413\n",
            "val Loss: 0.1815 Acc: 0.9343\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 4 out of 11\n",
            "Epoch 69/99\n",
            "----------\n",
            "train Loss: 0.1697 Acc: 0.9411\n",
            "val Loss: 0.1814 Acc: 0.9335\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 5 out of 11\n",
            "Epoch 70/99\n",
            "----------\n",
            "train Loss: 0.1701 Acc: 0.9403\n",
            "val Loss: 0.1809 Acc: 0.9344\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (0.181203 --> 0.180936).  Saving model ...\n",
            "Epoch 71/99\n",
            "----------\n",
            "train Loss: 0.1684 Acc: 0.9412\n",
            "val Loss: 0.1789 Acc: 0.9355\n",
            "Current learning rate : 0.00030000\n",
            "Validation loss decreased (0.180936 --> 0.178938).  Saving model ...\n",
            "Epoch 72/99\n",
            "----------\n",
            "train Loss: 0.1683 Acc: 0.9424\n",
            "val Loss: 0.1833 Acc: 0.9333\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 73/99\n",
            "----------\n",
            "train Loss: 0.1657 Acc: 0.9418\n",
            "val Loss: 0.1818 Acc: 0.9346\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 74/99\n",
            "----------\n",
            "train Loss: 0.1670 Acc: 0.9435\n",
            "val Loss: 0.1824 Acc: 0.9360\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 75/99\n",
            "----------\n",
            "train Loss: 0.1647 Acc: 0.9438\n",
            "val Loss: 0.1812 Acc: 0.9341\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 4 out of 11\n",
            "Epoch 76/99\n",
            "----------\n",
            "train Loss: 0.1656 Acc: 0.9426\n",
            "val Loss: 0.1793 Acc: 0.9345\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 5 out of 11\n",
            "Epoch 77/99\n",
            "----------\n",
            "train Loss: 0.1657 Acc: 0.9423\n",
            "val Loss: 0.1812 Acc: 0.9336\n",
            "Current learning rate : 0.00030000\n",
            "EarlyStopping counter: 6 out of 11\n",
            "Epoch 78/99\n",
            "----------\n",
            "train Loss: 0.1628 Acc: 0.9428\n",
            "val Loss: 0.1795 Acc: 0.9340\n",
            "Current learning rate : 0.00003000\n",
            "EarlyStopping counter: 7 out of 11\n",
            "Epoch 79/99\n",
            "----------\n",
            "train Loss: 0.1606 Acc: 0.9455\n",
            "val Loss: 0.1791 Acc: 0.9350\n",
            "Current learning rate : 0.00003000\n",
            "EarlyStopping counter: 8 out of 11\n",
            "Epoch 80/99\n",
            "----------\n",
            "train Loss: 0.1591 Acc: 0.9457\n",
            "val Loss: 0.1790 Acc: 0.9348\n",
            "Current learning rate : 0.00003000\n",
            "EarlyStopping counter: 9 out of 11\n",
            "Epoch 81/99\n",
            "----------\n",
            "train Loss: 0.1572 Acc: 0.9451\n",
            "val Loss: 0.1788 Acc: 0.9332\n",
            "Current learning rate : 0.00003000\n",
            "Validation loss decreased (0.178938 --> 0.178815).  Saving model ...\n",
            "Epoch 82/99\n",
            "----------\n",
            "train Loss: 0.1595 Acc: 0.9469\n",
            "val Loss: 0.1811 Acc: 0.9335\n",
            "Current learning rate : 0.00003000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 83/99\n",
            "----------\n",
            "train Loss: 0.1578 Acc: 0.9455\n",
            "val Loss: 0.1786 Acc: 0.9347\n",
            "Current learning rate : 0.00003000\n",
            "Validation loss decreased (0.178815 --> 0.178643).  Saving model ...\n",
            "Epoch 84/99\n",
            "----------\n",
            "train Loss: 0.1597 Acc: 0.9452\n",
            "val Loss: 0.1821 Acc: 0.9329\n",
            "Current learning rate : 0.00003000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 85/99\n",
            "----------\n",
            "train Loss: 0.1586 Acc: 0.9450\n",
            "val Loss: 0.1795 Acc: 0.9331\n",
            "Current learning rate : 0.00003000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 86/99\n",
            "----------\n",
            "train Loss: 0.1585 Acc: 0.9440\n",
            "val Loss: 0.1778 Acc: 0.9336\n",
            "Current learning rate : 0.00003000\n",
            "Validation loss decreased (0.178643 --> 0.177783).  Saving model ...\n",
            "Epoch 87/99\n",
            "----------\n",
            "train Loss: 0.1584 Acc: 0.9466\n",
            "val Loss: 0.1794 Acc: 0.9339\n",
            "Current learning rate : 0.00003000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 88/99\n",
            "----------\n",
            "train Loss: 0.1593 Acc: 0.9452\n",
            "val Loss: 0.1809 Acc: 0.9323\n",
            "Current learning rate : 0.00003000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 89/99\n",
            "----------\n",
            "train Loss: 0.1589 Acc: 0.9457\n",
            "val Loss: 0.1767 Acc: 0.9352\n",
            "Current learning rate : 0.00003000\n",
            "Validation loss decreased (0.177783 --> 0.176687).  Saving model ...\n",
            "Epoch 90/99\n",
            "----------\n",
            "train Loss: 0.1571 Acc: 0.9466\n",
            "val Loss: 0.1783 Acc: 0.9343\n",
            "Current learning rate : 0.00003000\n",
            "EarlyStopping counter: 1 out of 11\n",
            "Epoch 91/99\n",
            "----------\n",
            "train Loss: 0.1569 Acc: 0.9469\n",
            "val Loss: 0.1785 Acc: 0.9338\n",
            "Current learning rate : 0.00003000\n",
            "EarlyStopping counter: 2 out of 11\n",
            "Epoch 92/99\n",
            "----------\n",
            "train Loss: 0.1599 Acc: 0.9453\n",
            "val Loss: 0.1787 Acc: 0.9348\n",
            "Current learning rate : 0.00003000\n",
            "EarlyStopping counter: 3 out of 11\n",
            "Epoch 93/99\n",
            "----------\n",
            "train Loss: 0.1584 Acc: 0.9467\n",
            "val Loss: 0.1790 Acc: 0.9340\n",
            "Current learning rate : 0.00003000\n",
            "EarlyStopping counter: 4 out of 11\n",
            "Epoch 94/99\n",
            "----------\n",
            "train Loss: 0.1587 Acc: 0.9464\n",
            "val Loss: 0.1781 Acc: 0.9345\n",
            "Current learning rate : 0.00003000\n",
            "EarlyStopping counter: 5 out of 11\n",
            "Epoch 95/99\n",
            "----------\n",
            "train Loss: 0.1578 Acc: 0.9484\n",
            "val Loss: 0.1792 Acc: 0.9324\n",
            "Current learning rate : 0.00003000\n",
            "EarlyStopping counter: 6 out of 11\n",
            "Epoch 96/99\n",
            "----------\n",
            "train Loss: 0.1561 Acc: 0.9470\n",
            "val Loss: 0.1775 Acc: 0.9350\n",
            "Current learning rate : 0.00000300\n",
            "EarlyStopping counter: 7 out of 11\n",
            "Epoch 97/99\n",
            "----------\n",
            "train Loss: 0.1564 Acc: 0.9466\n",
            "val Loss: 0.1812 Acc: 0.9329\n",
            "Current learning rate : 0.00000300\n",
            "EarlyStopping counter: 8 out of 11\n",
            "Epoch 98/99\n",
            "----------\n",
            "train Loss: 0.1548 Acc: 0.9480\n",
            "val Loss: 0.1782 Acc: 0.9337\n",
            "Current learning rate : 0.00000300\n",
            "EarlyStopping counter: 9 out of 11\n",
            "Epoch 99/99\n",
            "----------\n",
            "train Loss: 0.1563 Acc: 0.9476\n",
            "val Loss: 0.1792 Acc: 0.9343\n",
            "Current learning rate : 0.00000300\n",
            "EarlyStopping counter: 10 out of 11\n",
            "Training complete in 68m 55s\n",
            "Best val Acc: 0.935989\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dVn5OTrKpPQ"
      },
      "source": [
        "torch.save(model, './CustomCNN_model.pth')\n",
        "import natsort as nt\n",
        "from PIL import Image\n",
        "from torch.autograd import Variable \n",
        "def test_model():\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.Resize([img_size,img_size]),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5])\n",
        "    ])\n",
        "    model_ft = torch.load('./CustomCNN_model.pth', map_location=device)\n",
        "    \n",
        "    path_test = os.path.join(os.getcwd(), 'test')\n",
        "    image_list = nt.natsorted(os.listdir(path_test))\n",
        "    output_list = []\n",
        "    for i, images in enumerate(image_list):\n",
        "        path_test_image = os.path.join(path_test, images)\n",
        "        image = Image.open(path_test_image)\n",
        "        image = data_transforms(image)\n",
        "        image.unsqueeze_(dim=0)\n",
        "        image = Variable(image)\n",
        "        image = image.cuda(device)\n",
        "        torch.no_grad()\n",
        "        output = model(image)\n",
        "        #print(output)\n",
        "        output = torch.argmax(output, dim=1)\n",
        "        \n",
        "        output_list.append(output)\n",
        "        \n",
        "    return output_list\n",
        "output = test_model()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzsWQi2fV8MI",
        "outputId": "72a929e1-afc3-4b66-eb95-8b417c2f1663"
      },
      "source": [
        "\n",
        "\n",
        "submit.label = torch.cat(output).detach().cpu().numpy()\n",
        "print(submit)\n",
        "submit.to_csv('./CustomCNN_result.csv', index=False)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         id  label\n",
            "0        11      7\n",
            "1        15      0\n",
            "2        17      3\n",
            "3        21      6\n",
            "4        22      3\n",
            "...     ...    ...\n",
            "8455  59970      6\n",
            "8456  59971      4\n",
            "8457  59982      4\n",
            "8458  59986      5\n",
            "8459  59996      1\n",
            "\n",
            "[8460 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1z7SJc8WLGq",
        "outputId": "163b0e89-20d2-4703-c86b-d281128cbc3b"
      },
      "source": [
        "!kaggle competitions submit -c sejong-ai-challenge-p1 -f './CustomCNN_result.csv' -m \"Message\""
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.6)\n",
            "100% 64.5k/64.5k [00:01<00:00, 35.1kB/s]\n",
            "Successfully submitted to Sejong AI Challenge 문제1"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHVRbJvebuP-"
      },
      "source": [
        "# 뭐지 0.26 나오는데  . . ?"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}