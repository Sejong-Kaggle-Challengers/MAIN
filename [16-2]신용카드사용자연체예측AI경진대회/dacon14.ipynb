{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_columns',50)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/daconcard/open/train.csv')\ntest=pd.read_csv('/kaggle/input/daconcard/open/test.csv')\nsubmit=pd.read_csv('/kaggle/input/daconcard/open/sample_submission.csv')","metadata":{"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def datainfo(df):\n    return pd.DataFrame([(col,df[col].dtype,df[col].isna().sum(),df[col].nunique(),df[col].unique()[:5]) for col in df.columns],\n                       columns=['name','dtype','missing','nunique','values :5'])","metadata":{"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"datainfo(train)","metadata":{"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"             name    dtype  missing  nunique  \\\n0           index    int64        0    26457   \n1          gender   object        0        2   \n2             car   object        0        2   \n3         reality   object        0        2   \n4       child_num    int64        0        9   \n5    income_total  float64        0      249   \n6     income_type   object        0        5   \n7        edu_type   object        0        5   \n8     family_type   object        0        5   \n9      house_type   object        0        6   \n10     DAYS_BIRTH    int64        0     6621   \n11  DAYS_EMPLOYED    int64        0     3470   \n12     FLAG_MOBIL    int64        0        1   \n13     work_phone    int64        0        2   \n14          phone    int64        0        2   \n15          email    int64        0        2   \n16     occyp_type   object     8171       18   \n17    family_size  float64        0       10   \n18    begin_month  float64        0       61   \n19         credit  float64        0        3   \n\n                                            values :5  \n0                                     [0, 1, 2, 3, 4]  \n1                                              [F, M]  \n2                                              [N, Y]  \n3                                              [N, Y]  \n4                                     [0, 1, 2, 3, 4]  \n5   [202500.0, 247500.0, 450000.0, 157500.0, 27000...  \n6   [Commercial associate, Working, State servant,...  \n7   [Higher education, Secondary / secondary speci...  \n8   [Married, Civil marriage, Separated, Single / ...  \n9   [Municipal apartment, House / apartment, With ...  \n10           [-13899, -11380, -19087, -15088, -15037]  \n11                [-4709, -1540, -4434, -2092, -2105]  \n12                                                [1]  \n13                                             [0, 1]  \n14                                             [0, 1]  \n15                                             [0, 1]  \n16  [nan, Laborers, Managers, Sales staff, High sk...  \n17                          [2.0, 3.0, 4.0, 1.0, 5.0]  \n18                  [-6.0, -5.0, -22.0, -37.0, -26.0]  \n19                                    [1.0, 2.0, 0.0]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>dtype</th>\n      <th>missing</th>\n      <th>nunique</th>\n      <th>values :5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>index</td>\n      <td>int64</td>\n      <td>0</td>\n      <td>26457</td>\n      <td>[0, 1, 2, 3, 4]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>gender</td>\n      <td>object</td>\n      <td>0</td>\n      <td>2</td>\n      <td>[F, M]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>car</td>\n      <td>object</td>\n      <td>0</td>\n      <td>2</td>\n      <td>[N, Y]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>reality</td>\n      <td>object</td>\n      <td>0</td>\n      <td>2</td>\n      <td>[N, Y]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>child_num</td>\n      <td>int64</td>\n      <td>0</td>\n      <td>9</td>\n      <td>[0, 1, 2, 3, 4]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>income_total</td>\n      <td>float64</td>\n      <td>0</td>\n      <td>249</td>\n      <td>[202500.0, 247500.0, 450000.0, 157500.0, 27000...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>income_type</td>\n      <td>object</td>\n      <td>0</td>\n      <td>5</td>\n      <td>[Commercial associate, Working, State servant,...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>edu_type</td>\n      <td>object</td>\n      <td>0</td>\n      <td>5</td>\n      <td>[Higher education, Secondary / secondary speci...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>family_type</td>\n      <td>object</td>\n      <td>0</td>\n      <td>5</td>\n      <td>[Married, Civil marriage, Separated, Single / ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>house_type</td>\n      <td>object</td>\n      <td>0</td>\n      <td>6</td>\n      <td>[Municipal apartment, House / apartment, With ...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>DAYS_BIRTH</td>\n      <td>int64</td>\n      <td>0</td>\n      <td>6621</td>\n      <td>[-13899, -11380, -19087, -15088, -15037]</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>DAYS_EMPLOYED</td>\n      <td>int64</td>\n      <td>0</td>\n      <td>3470</td>\n      <td>[-4709, -1540, -4434, -2092, -2105]</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>FLAG_MOBIL</td>\n      <td>int64</td>\n      <td>0</td>\n      <td>1</td>\n      <td>[1]</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>work_phone</td>\n      <td>int64</td>\n      <td>0</td>\n      <td>2</td>\n      <td>[0, 1]</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>phone</td>\n      <td>int64</td>\n      <td>0</td>\n      <td>2</td>\n      <td>[0, 1]</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>email</td>\n      <td>int64</td>\n      <td>0</td>\n      <td>2</td>\n      <td>[0, 1]</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>occyp_type</td>\n      <td>object</td>\n      <td>8171</td>\n      <td>18</td>\n      <td>[nan, Laborers, Managers, Sales staff, High sk...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>family_size</td>\n      <td>float64</td>\n      <td>0</td>\n      <td>10</td>\n      <td>[2.0, 3.0, 4.0, 1.0, 5.0]</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>begin_month</td>\n      <td>float64</td>\n      <td>0</td>\n      <td>61</td>\n      <td>[-6.0, -5.0, -22.0, -37.0, -26.0]</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>credit</td>\n      <td>float64</td>\n      <td>0</td>\n      <td>3</td>\n      <td>[1.0, 2.0, 0.0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"## ordinal : edu_type(Lower,Secondary,Incomplete,Higher,academ),child_num() income(Student,)\n## binary : gender,car,reality,work_phone,phone,email,\n## nominal : ","metadata":{"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"datainfo(test)","metadata":{"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"             name    dtype  missing  nunique  \\\n0           index    int64        0    10000   \n1          gender   object        0        2   \n2             car   object        0        2   \n3         reality   object        0        2   \n4       child_num    int64        0        6   \n5    income_total  float64        0      208   \n6     income_type   object        0        5   \n7        edu_type   object        0        5   \n8     family_type   object        0        5   \n9      house_type   object        0        6   \n10     DAYS_BIRTH    int64        0     4675   \n11  DAYS_EMPLOYED    int64        0     2665   \n12     FLAG_MOBIL    int64        0        1   \n13     work_phone    int64        0        2   \n14          phone    int64        0        2   \n15          email    int64        0        2   \n16     occyp_type   object     3152       18   \n17    family_size  float64        0        7   \n18    begin_month  float64        0       61   \n\n                                            values :5  \n0                 [26457, 26458, 26459, 26460, 26461]  \n1                                              [M, F]  \n2                                              [Y, N]  \n3                                              [N, Y]  \n4                                     [0, 1, 2, 3, 5]  \n5   [112500.0, 135000.0, 69372.0, 225000.0, 126000.0]  \n6   [Pensioner, State servant, Working, Commercial...  \n7   [Secondary / secondary special, Higher educati...  \n8   [Civil marriage, Married, Single / not married...  \n9   [House / apartment, With parents, Rented apart...  \n10           [-21990, -18964, -15887, -19270, -17822]  \n11                [365243, -8671, -217, -2531, -9385]  \n12                                                [1]  \n13                                             [0, 1]  \n14                                             [1, 0]  \n15                                             [0, 1]  \n16     [nan, Core staff, Laborers, Drivers, Managers]  \n17                          [2.0, 1.0, 3.0, 4.0, 5.0]  \n18                 [-60.0, -36.0, -40.0, -41.0, -8.0]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>dtype</th>\n      <th>missing</th>\n      <th>nunique</th>\n      <th>values :5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>index</td>\n      <td>int64</td>\n      <td>0</td>\n      <td>10000</td>\n      <td>[26457, 26458, 26459, 26460, 26461]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>gender</td>\n      <td>object</td>\n      <td>0</td>\n      <td>2</td>\n      <td>[M, F]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>car</td>\n      <td>object</td>\n      <td>0</td>\n      <td>2</td>\n      <td>[Y, N]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>reality</td>\n      <td>object</td>\n      <td>0</td>\n      <td>2</td>\n      <td>[N, Y]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>child_num</td>\n      <td>int64</td>\n      <td>0</td>\n      <td>6</td>\n      <td>[0, 1, 2, 3, 5]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>income_total</td>\n      <td>float64</td>\n      <td>0</td>\n      <td>208</td>\n      <td>[112500.0, 135000.0, 69372.0, 225000.0, 126000.0]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>income_type</td>\n      <td>object</td>\n      <td>0</td>\n      <td>5</td>\n      <td>[Pensioner, State servant, Working, Commercial...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>edu_type</td>\n      <td>object</td>\n      <td>0</td>\n      <td>5</td>\n      <td>[Secondary / secondary special, Higher educati...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>family_type</td>\n      <td>object</td>\n      <td>0</td>\n      <td>5</td>\n      <td>[Civil marriage, Married, Single / not married...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>house_type</td>\n      <td>object</td>\n      <td>0</td>\n      <td>6</td>\n      <td>[House / apartment, With parents, Rented apart...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>DAYS_BIRTH</td>\n      <td>int64</td>\n      <td>0</td>\n      <td>4675</td>\n      <td>[-21990, -18964, -15887, -19270, -17822]</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>DAYS_EMPLOYED</td>\n      <td>int64</td>\n      <td>0</td>\n      <td>2665</td>\n      <td>[365243, -8671, -217, -2531, -9385]</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>FLAG_MOBIL</td>\n      <td>int64</td>\n      <td>0</td>\n      <td>1</td>\n      <td>[1]</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>work_phone</td>\n      <td>int64</td>\n      <td>0</td>\n      <td>2</td>\n      <td>[0, 1]</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>phone</td>\n      <td>int64</td>\n      <td>0</td>\n      <td>2</td>\n      <td>[1, 0]</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>email</td>\n      <td>int64</td>\n      <td>0</td>\n      <td>2</td>\n      <td>[0, 1]</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>occyp_type</td>\n      <td>object</td>\n      <td>3152</td>\n      <td>18</td>\n      <td>[nan, Core staff, Laborers, Drivers, Managers]</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>family_size</td>\n      <td>float64</td>\n      <td>0</td>\n      <td>7</td>\n      <td>[2.0, 1.0, 3.0, 4.0, 5.0]</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>begin_month</td>\n      <td>float64</td>\n      <td>0</td>\n      <td>61</td>\n      <td>[-60.0, -36.0, -40.0, -41.0, -8.0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"datainfo(submit)","metadata":{"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"    name  dtype  missing  nunique                            values :5\n0  index  int64        0    10000  [26457, 26458, 26459, 26460, 26461]\n1      0  int64        0        1                                  [0]\n2      1  int64        0        1                                  [0]\n3      2  int64        0        1                                  [0]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>dtype</th>\n      <th>missing</th>\n      <th>nunique</th>\n      <th>values :5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>index</td>\n      <td>int64</td>\n      <td>0</td>\n      <td>10000</td>\n      <td>[26457, 26458, 26459, 26460, 26461]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>int64</td>\n      <td>0</td>\n      <td>1</td>\n      <td>[0]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>int64</td>\n      <td>0</td>\n      <td>1</td>\n      <td>[0]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>int64</td>\n      <td>0</td>\n      <td>1</td>\n      <td>[0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"## 제거 \ntrain=train.drop(['FLAG_MOBIL','index'],axis=1)\ntest=test.drop(['FLAG_MOBIL','index'],axis=1)\n\n## 중복 확인 및 제거 -> 안하는게 성능 굳.\n# cols=['gender', 'car', 'reality', 'child_num', 'income_total',\n#       'income_type', 'edu_type', 'family_type', 'house_type', 'DAYS_BIRTH',\n#       'DAYS_EMPLOYED','work_phone', 'phone', 'email',\n#       'occyp_type', 'family_size','begin_month']\n\n## 마지막에 입력된 데이터가 바뀌어도 진짜라고 가정. drop 안했을 때 성능 굳\n# train=train.drop_duplicates(subset=cols,keep='last')\n\n## 고용일 처리 -> 양수 성능 굳 ? \n# train['DAYS_BIRTH']=train['DAYS_BIRTH']*(-1/365)\n# test['DAYS_BIRTH']=test['DAYS_BIRTH']*(-1/365)\n\n# train['DAYS_EMPLOYED']=train['DAYS_EMPLOYED']*(-1/365)\n# train.loc[train['DAYS_EMPLOYED']<0,'DAYS_EMPLOYED']=-999\n# test['DAYS_EMPLOYED']=test['DAYS_EMPLOYED']*(-1/365)\n# test.loc[test['DAYS_EMPLOYED']<0,'DAYS_EMPLOYED']=-999\n\n## 가족수 이상치 나이대로 판별 이상 판단.\ntrain=train[(train['family_size']!=20)&(train['family_size']!=15)]\n# 결측값 \n# train['occyp_type']=train['occyp_type'].fillna('unknown')\n# test['occyp_type']=test['occyp_type'].fillna('unknown')\n\ntrain.fillna('NAN', inplace=True)\ntrain['occyp_type'].loc[(train.occyp_type == 'NAN')&(train.DAYS_EMPLOYED > 0)]='Unemployed'\ntrain['occyp_type'].loc[(train.occyp_type == 'NAN')&(train.DAYS_EMPLOYED < 0)]='Missing'\n\ntest.fillna('NAN', inplace=True)\ntest['occyp_type'].loc[(test.occyp_type == 'NAN')&(test.DAYS_EMPLOYED > 0)]='Unemployed'\ntest['occyp_type'].loc[(test.occyp_type == 'NAN')&(test.DAYS_EMPLOYED < 0)]='Missing'","metadata":{"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_block(indexer, value, name)\n/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_block(indexer, value, name)\n/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_block(indexer, value, name)\n/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._setitem_single_block(indexer, value, name)\n","output_type":"stream"}]},{"cell_type":"code","source":"## 발급수 파생변수 추가 20210511\n\ncols=['gender', 'car', 'reality', 'child_num', 'income_total',\n      'income_type', 'edu_type', 'family_type', 'house_type', 'DAYS_BIRTH',\n      'DAYS_EMPLOYED','work_phone', 'phone', 'email',\n      'occyp_type', 'family_size']\n\ntrain['count']=1\ntest['count']=1\n\ntr_grp_count=train.groupby(cols,as_index=False).agg({'count':[('count','count')]})\nts_grp_count=test.groupby(cols,as_index=False).agg({'count':[('count','count')]})","metadata":{"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"colnames=[]\nfor col,_ in tr_grp_count.columns:\n    colnames.append(col)\n    \ntr_grp_count.columns=colnames\nts_grp_count.columns=colnames\n\ntrain=pd.merge(train,tr_grp_count,on=cols,how='left')\ntest=pd.merge(test,ts_grp_count,on=cols,how='left')","metadata":{"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"train=train.drop('count_x',axis=1)\ntrain=train.rename(columns={'count_y':'count'})\ntest=test.drop('count_x',axis=1)\ntest=test.rename(columns={'count_y':'count'})","metadata":{"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"## train drop first x 20210511\n# cols=['gender', 'car', 'reality', 'child_num', 'income_total',\n#       'income_type', 'edu_type', 'family_type', 'house_type', 'DAYS_BIRTH',\n#       'DAYS_EMPLOYED','work_phone', 'phone', 'email',\n#       'occyp_type', 'family_size']\n\n# tr_idx=train.drop_duplicates(cols,keep='first').index\n# train=train.drop(tr_idx,axis=0)","metadata":{"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# train['DAYS_BIRTH']=train['DAYS_BIRTH']*(-1/365)\n# test['DAYS_BIRTH']=test['DAYS_BIRTH']*(-1/365)\n\n# train['DAYS_BIRTH']=pd.cut(train['DAYS_BIRTH'],bins=[20,30,40,50,60,70],labels=['20s','30s','40s','50s','60s'])\n\n# train['DAYS_EMPLOYED']=train['DAYS_EMPLOYED']*(-1/365)\n# # train.loc[train['DAYS_EMPLOYED']<0,'DAYS_EMPLOYED']=-999\n# test['DAYS_EMPLOYED']=test['DAYS_EMPLOYED']*(-1/365)\n# # test.loc[test['DAYS_EMPLOYED']<0,'DAYS_EMPLOYED']=-999\n\n# train['DAYS_EMPLOYED']=pd.cut(train['DAYS_EMPLOYED'],bins=[-10000,0,1,5,10,20,10000],labels=['nan','1y','5y','10y','20y','>20y'])","metadata":{"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# birth_order = {\n#     '<20' : 0, \n#     '20s' : 1, \n#     '30s' : 2,\n#     '40s' : 3, \n#     '50s' : 4 ,\n#     '60s':5,\n# }\n# train.DAYS_BIRTH = train.DAYS_BIRTH.map(birth_order)\n# test.DAYS_BIRTH = test.DAYS_BIRTH.map(birth_order)","metadata":{"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# emp_order = {\n#     'nan' : 0, \n#     '1y' : 1, \n#     '5y' : 2,\n#     '10y' : 3, \n#     '20y' : 4,\n#     '>20y':5\n# }\n# train.DAYS_EMPLOYED = train.DAYS_EMPLOYED.map(emp_order)\n# test.DAYS_EMPLOYED = test.DAYS_EMPLOYED.map(emp_order)\n","metadata":{"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# edu_order = {\n#     'Lower secondary' : 0, \n#     'Secondary / secondary special' : 1, \n#     'Incomplete higher' : 2,\n#     'Higher education' : 3, \n#     'Academic degree' : 4 \n# }\n# train.edu_type = train.edu_type.map(edu_order)\n# test.edu_type = test.edu_type.map(edu_order)\n\n# # train.groupby('income_type')['income_total'].median()\n\n# income_order={\n#     'Pensioner':0,\n#     'Working':1,\n#     'Student':2,\n#     'State servant':3,\n#     'Commercial associate':4\n# }\n# train.income_type=train.income_type.map(income_order)\n# test.income_type=test.income_type.map(income_order)\n\n# # train.groupby('family_type')['income_total'].median()\n\n# family_order={\n#     'Civil marriage':0,\n#     'Separated':0,\n#     'Married':1,\n#     'Widow':1,\n#     'Single / not married':2\n# }\n# train.family_type=train.family_type.map(family_order)\n# test.family_type=test.family_type.map(family_order)\n\n# # train.groupby('house_type')['income_total'].median()\n\n# house_order={\n#     'House / apartment':0,\n#     'Municipal apartment':0,\n#     'With parents':0,\n#     'Rented apartment':1,\n#     'Office apartment':1,\n#     'Co-op apartment':2\n# }\n# train.house_type=train.house_type.map(house_order)\n# test.house_type=test.house_type.map(house_order)\n\n# # train.groupby('occyp_type')['income_total'].median()\n# ## co-op 는 가격의 차이가 있다는 가정.\n\n# occyp_order={\n#     'unknown':0,\n#     'Low-skill Laborers':1,\n#     'Secretaries':2,\n#     'Cleaning staff':2,\n#     'Cooking staff':2,\n#     'HR staff':3,\n#     'Laborers':3,\n#     'Medicine staff':3,\n#     'Sales staff':3,\n#     'Security staff':3,\n#     'Waiters/barmen staff':3,\n#     'Private service staff':4,\n#     'High skill tech staff':4,\n#     'Accountants':4,\n#     'Core staff':4,\n#     'IT staff':4,\n#     'Realty agents':5,\n#     'Drivers':5,\n#     'Managers':5\n# }\n# train.occyp_type=train.occyp_type.map(occyp_order)\n# test.occyp_type=test.occyp_type.map(occyp_order)\n\n# train['gender']=train['gender'].apply(lambda x:1 if x=='M' else 0)\n# train['car']=train['car'].apply(lambda x:1 if x=='N' else 0)\n# train['reality']=train['reality'].apply(lambda x:1 if x=='N' else 0)\n\n# test['gender']=test['gender'].apply(lambda x:1 if x=='M' else 0)\n# test['car']=test['car'].apply(lambda x:1 if x=='N' else 0)\n# test['reality']=test['reality'].apply(lambda x:1 if x=='N' else 0)","metadata":{"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# train['begin_month']*=(-1/12)\n# test['begin_month']*=(-1/12)\n\n# train['begin_month']=pd.cut(train['begin_month'],bins=[-1,0.5,1.1,2.1,3.1,4.1,5.1],labels=['0y','1/2y','1y','2y','3y','4y'])\n# test['begin_month']=pd.cut(test['begin_month'],bins=[-1,0.5,1.1,2.1,3.1,4.1,5.1],labels=['0y','1/2y','1y','2y','3y','4y'])","metadata":{"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# begin_order={\n#     '0y':0,\n#     '1/2y':1,\n#     '1y':2,\n#     '2y':3,\n#     '3y':4,\n#     '4y':5\n# }\n# train.begin_month=train.begin_month.map(begin_order)\n# test.begin_month=test.begin_month.map(begin_order)\n\n# train['begin_order']=train['begin_month'].astype(int)\n# test['begin_order']=test['begin_month'].astype(int)","metadata":{"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"## 시도 파생변수 : 성능 더 떨어진다.\n\n# train['family_size']=pd.Categorical(train['family_size'],ordered=True)\n# train['child_num']=pd.Categorical(train['child_num'],ordered=True)\n# train['begin_month']=pd.Categorical(train['begin_month'],ordered=True)\n\n# test['family_size']=pd.Categorical(test['family_size'],ordered=True)\n# test['child_num']=pd.Categorical(test['child_num'],ordered=True)\n# test['begin_month']=pd.Categorical(test['begin_month'],ordered=True)","metadata":{"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"## income과 gender에 따라 차이가 없다.\n# sns.catplot(\n#     data=train, kind=\"bar\",\n#     x=\"credit\", y=\"income_total\", hue=\"gender\",\n#     ci=\"sd\", palette=\"dark\", alpha=.6, height=6\n# )\n\n## car reality에 따라 income의 차이는 있다.\n# sns.catplot(\n#     data=train, kind=\"bar\",\n#     x=\"car\", y=\"income_total\", hue=\"reality\",\n#     ci=\"sd\", palette=\"dark\", alpha=.6, height=6\n# )\n\n## car reality도 차이없다. \n# sns.catplot(\n#     data=train, kind=\"bar\",\n#     x=\"credit\", y=\"income_total\", hue=\"reality\",\n#     ci=\"sd\", palette=\"dark\", alpha=.6, height=6\n# )\n\n## income_total은 제거 해본다.\n# sns.catplot(\n#     data=train, kind=\"bar\",\n#     x=\"credit\", y=\"income_total\", hue=\"begin_month\",\n#     ci=\"sd\", palette=\"dark\", alpha=.6, height=6\n# )","metadata":{"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"## #했을때 비슷.\n# train['gender']=train['gender'].apply(lambda x:1 if x=='M' else 0)\n# train['car']=train['car'].apply(lambda x:1 if x=='N' else 0)\n# train['reality']=train['reality'].apply(lambda x:1 if x=='N' else 0)\n# train['work_phone']=train['work_phone'].apply(lambda x:1 if x==0 else 0)\n# train['phone']=train['phone'].apply(lambda x:1 if x==0 else 0)\n# train['email']=train['email'].apply(lambda x:1 if x==0 else 0)\n\n# test['gender']=test['gender'].apply(lambda x:1 if x=='M' else 0)\n# test['car']=test['car'].apply(lambda x:1 if x=='N' else 0)\n# test['reality']=test['reality'].apply(lambda x:1 if x=='N' else 0)\n# test['work_phone']=test['work_phone'].apply(lambda x:1 if x==0 else 0)\n# test['phone']=test['phone'].apply(lambda x:1 if x==0 else 0)\n# test['email']=test['email'].apply(lambda x:1 if x==0 else 0)","metadata":{"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# tr_cat_dum=pd.get_dummies(train[['income_type','edu_type','family_type','house_type','occyp_type']],drop_first=True)\n# ts_cat_dum=pd.get_dummies(test[['income_type','edu_type','family_type','house_type','occyp_type']],drop_first=True)","metadata":{"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# train=train.drop(['income_type','edu_type','family_type','house_type','occyp_type'],axis=1)\n# test=test.drop(['income_type','edu_type','family_type','house_type','occyp_type'],axis=1)","metadata":{"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# train=pd.concat([train,tr_cat_dum],axis=1)\n# test=pd.concat([test,ts_cat_dum],axis=1)","metadata":{"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"## label\n## 수가 많아서 이걸로.\n\n# from category_encoders.ordinal import OrdinalEncoder\n\n# oe=OrdinalEncoder(handle_missing=-999)\n\n# oe.fit(train['family#begin'])\n# train['family#begin']=oe.transform(train['family#begin'])\n# test['family#begin']=oe.transform(test['family#begin'])","metadata":{"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"## 가설1. 다중공선성\n## 가설2. income_total 구분.\n# train=train.drop(['DAYS_BIRTH','child_num'],axis=1)\n# test=test.drop(['DAYS_BIRTH','child_num'],axis=1)\n\n# train['income_total']=pd.cut(train['income_total'],bins=[0,100000,350000,500000,750000,1000000,2000000],labels=['F','E','D','C','B','A'])\n# test['income_total']=pd.cut(test['income_total'],bins=[0,100000,350000,500000,750000,1000000,2000000],labels=['F','E','D','C','B','A'])\\","metadata":{"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"## feature importance에는 좋다고 나오는데 성능은 1.2 뭐가 문제일까.\n## 1. + - 실험. 과적합\n## 2. * 실험 뭔가 맞춰지는. 다른것과 시너지가 있는것 같다.\n# train['child*family']=train.apply(lambda row:row['child_num']*row['family_size'],axis=1)\n# test['child*family']=test.apply(lambda row:row['child_num']*row['family_size'],axis=1)\n\n# train=train.drop(['family_size','child_num'],axis=1)\n# test=test.drop(['family_size','child_num'],axis=1)","metadata":{"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"# cat_cols=['gender','car','reality','income_type','edu_type','family_type',\n#           'house_type','work_phone','phone','email','occyp_type','child*family',\n#           'DAYS_EMPLOYED','DAYS_BIRTH']\n\n# from itertools import combinations as comb\n\n# for a,b in comb(cat_cols,2):\n#     df=train.groupby([a,b,'begin_month'],as_index=False)['income_total'].mean()\n#     df.columns=[a,b,'begin_month',f'{a}_{b}_grpmean']\n#     train=pd.merge(train,df,how='left',on=[a,b,'begin_month'])\n#     test=pd.merge(test,df,how='left',on=[a,b,'begin_month'])","metadata":{"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# drop_cols=[]\n# for col in test.columns:\n#     if test[col].isnull().sum():\n#         drop_cols.append(col)","metadata":{"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"# train=train.drop(drop_cols,axis=1)\n# test=test.drop(drop_cols,axis=1)","metadata":{"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"train=pd.get_dummies(train,drop_first=True)\ntest=pd.get_dummies(test,drop_first=True)","metadata":{"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"train.shape,test.shape","metadata":{"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"((26453, 50), (10000, 49))"},"metadata":{}}]},{"cell_type":"code","source":"# train['begin_month']=train['begin_month'].astype(int)","metadata":{"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"X=train.drop('credit',axis=1)\ny=train['credit']\n\nfrom sklearn.preprocessing import normalize\n\nX=normalize(X,axis=1)\ntestc=test.copy()\ntestc=normalize(testc,axis=1)","metadata":{"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.15,random_state=71,stratify=y)","metadata":{"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"## xgb","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n\nparams={\n    'objective':'multi:softprob',\n    'random_state':71,\n    'n_estimators':1000\n}\nxgb_model=xgb.XGBClassifier(**params)\n\nxgb_model.fit(X_train,y_train,\n              eval_set=[(X_train, y_train),(X_test,y_test)],\n              eval_metric='mlogloss',verbose=True,\n              early_stopping_rounds=10)","metadata":{"scrolled":true,"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"[0]\tvalidation_0-mlogloss:0.97266\tvalidation_1-mlogloss:0.97695\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[1]\tvalidation_0-mlogloss:0.89947\tvalidation_1-mlogloss:0.90629\n[2]\tvalidation_0-mlogloss:0.85394\tvalidation_1-mlogloss:0.86406\n[3]\tvalidation_0-mlogloss:0.82409\tvalidation_1-mlogloss:0.83773\n[4]\tvalidation_0-mlogloss:0.80363\tvalidation_1-mlogloss:0.82114\n[5]\tvalidation_0-mlogloss:0.78923\tvalidation_1-mlogloss:0.81091\n[6]\tvalidation_0-mlogloss:0.77885\tvalidation_1-mlogloss:0.80300\n[7]\tvalidation_0-mlogloss:0.77144\tvalidation_1-mlogloss:0.79814\n[8]\tvalidation_0-mlogloss:0.76529\tvalidation_1-mlogloss:0.79468\n[9]\tvalidation_0-mlogloss:0.76070\tvalidation_1-mlogloss:0.79308\n[10]\tvalidation_0-mlogloss:0.75674\tvalidation_1-mlogloss:0.79126\n[11]\tvalidation_0-mlogloss:0.75372\tvalidation_1-mlogloss:0.78985\n[12]\tvalidation_0-mlogloss:0.75036\tvalidation_1-mlogloss:0.78840\n[13]\tvalidation_0-mlogloss:0.74549\tvalidation_1-mlogloss:0.78685\n[14]\tvalidation_0-mlogloss:0.74301\tvalidation_1-mlogloss:0.78614\n[15]\tvalidation_0-mlogloss:0.73639\tvalidation_1-mlogloss:0.78375\n[16]\tvalidation_0-mlogloss:0.73263\tvalidation_1-mlogloss:0.78295\n[17]\tvalidation_0-mlogloss:0.73060\tvalidation_1-mlogloss:0.78223\n[18]\tvalidation_0-mlogloss:0.72803\tvalidation_1-mlogloss:0.78144\n[19]\tvalidation_0-mlogloss:0.72557\tvalidation_1-mlogloss:0.78053\n[20]\tvalidation_0-mlogloss:0.72374\tvalidation_1-mlogloss:0.77929\n[21]\tvalidation_0-mlogloss:0.72067\tvalidation_1-mlogloss:0.77845\n[22]\tvalidation_0-mlogloss:0.71753\tvalidation_1-mlogloss:0.77723\n[23]\tvalidation_0-mlogloss:0.71473\tvalidation_1-mlogloss:0.77601\n[24]\tvalidation_0-mlogloss:0.71261\tvalidation_1-mlogloss:0.77497\n[25]\tvalidation_0-mlogloss:0.70718\tvalidation_1-mlogloss:0.77258\n[26]\tvalidation_0-mlogloss:0.70566\tvalidation_1-mlogloss:0.77218\n[27]\tvalidation_0-mlogloss:0.70079\tvalidation_1-mlogloss:0.77074\n[28]\tvalidation_0-mlogloss:0.69776\tvalidation_1-mlogloss:0.77015\n[29]\tvalidation_0-mlogloss:0.69516\tvalidation_1-mlogloss:0.77019\n[30]\tvalidation_0-mlogloss:0.69299\tvalidation_1-mlogloss:0.76947\n[31]\tvalidation_0-mlogloss:0.68883\tvalidation_1-mlogloss:0.76901\n[32]\tvalidation_0-mlogloss:0.68599\tvalidation_1-mlogloss:0.76899\n[33]\tvalidation_0-mlogloss:0.68454\tvalidation_1-mlogloss:0.76868\n[34]\tvalidation_0-mlogloss:0.68086\tvalidation_1-mlogloss:0.76841\n[35]\tvalidation_0-mlogloss:0.67742\tvalidation_1-mlogloss:0.76720\n[36]\tvalidation_0-mlogloss:0.67514\tvalidation_1-mlogloss:0.76614\n[37]\tvalidation_0-mlogloss:0.67331\tvalidation_1-mlogloss:0.76560\n[38]\tvalidation_0-mlogloss:0.67186\tvalidation_1-mlogloss:0.76513\n[39]\tvalidation_0-mlogloss:0.67015\tvalidation_1-mlogloss:0.76465\n[40]\tvalidation_0-mlogloss:0.66610\tvalidation_1-mlogloss:0.76379\n[41]\tvalidation_0-mlogloss:0.66319\tvalidation_1-mlogloss:0.76255\n[42]\tvalidation_0-mlogloss:0.66169\tvalidation_1-mlogloss:0.76244\n[43]\tvalidation_0-mlogloss:0.65814\tvalidation_1-mlogloss:0.76176\n[44]\tvalidation_0-mlogloss:0.65556\tvalidation_1-mlogloss:0.76134\n[45]\tvalidation_0-mlogloss:0.65308\tvalidation_1-mlogloss:0.76071\n[46]\tvalidation_0-mlogloss:0.65071\tvalidation_1-mlogloss:0.75961\n[47]\tvalidation_0-mlogloss:0.64823\tvalidation_1-mlogloss:0.75862\n[48]\tvalidation_0-mlogloss:0.64720\tvalidation_1-mlogloss:0.75812\n[49]\tvalidation_0-mlogloss:0.64631\tvalidation_1-mlogloss:0.75811\n[50]\tvalidation_0-mlogloss:0.64379\tvalidation_1-mlogloss:0.75730\n[51]\tvalidation_0-mlogloss:0.63919\tvalidation_1-mlogloss:0.75622\n[52]\tvalidation_0-mlogloss:0.63689\tvalidation_1-mlogloss:0.75519\n[53]\tvalidation_0-mlogloss:0.63589\tvalidation_1-mlogloss:0.75524\n[54]\tvalidation_0-mlogloss:0.63493\tvalidation_1-mlogloss:0.75531\n[55]\tvalidation_0-mlogloss:0.63294\tvalidation_1-mlogloss:0.75494\n[56]\tvalidation_0-mlogloss:0.63062\tvalidation_1-mlogloss:0.75431\n[57]\tvalidation_0-mlogloss:0.62944\tvalidation_1-mlogloss:0.75412\n[58]\tvalidation_0-mlogloss:0.62602\tvalidation_1-mlogloss:0.75323\n[59]\tvalidation_0-mlogloss:0.62221\tvalidation_1-mlogloss:0.75202\n[60]\tvalidation_0-mlogloss:0.62047\tvalidation_1-mlogloss:0.75145\n[61]\tvalidation_0-mlogloss:0.61892\tvalidation_1-mlogloss:0.75149\n[62]\tvalidation_0-mlogloss:0.61668\tvalidation_1-mlogloss:0.75049\n[63]\tvalidation_0-mlogloss:0.61556\tvalidation_1-mlogloss:0.74995\n[64]\tvalidation_0-mlogloss:0.61345\tvalidation_1-mlogloss:0.74951\n[65]\tvalidation_0-mlogloss:0.61073\tvalidation_1-mlogloss:0.74943\n[66]\tvalidation_0-mlogloss:0.60920\tvalidation_1-mlogloss:0.74932\n[67]\tvalidation_0-mlogloss:0.60709\tvalidation_1-mlogloss:0.74851\n[68]\tvalidation_0-mlogloss:0.60253\tvalidation_1-mlogloss:0.74803\n[69]\tvalidation_0-mlogloss:0.59959\tvalidation_1-mlogloss:0.74727\n[70]\tvalidation_0-mlogloss:0.59825\tvalidation_1-mlogloss:0.74689\n[71]\tvalidation_0-mlogloss:0.59688\tvalidation_1-mlogloss:0.74668\n[72]\tvalidation_0-mlogloss:0.59538\tvalidation_1-mlogloss:0.74528\n[73]\tvalidation_0-mlogloss:0.59228\tvalidation_1-mlogloss:0.74428\n[74]\tvalidation_0-mlogloss:0.59102\tvalidation_1-mlogloss:0.74388\n[75]\tvalidation_0-mlogloss:0.58910\tvalidation_1-mlogloss:0.74355\n[76]\tvalidation_0-mlogloss:0.58734\tvalidation_1-mlogloss:0.74311\n[77]\tvalidation_0-mlogloss:0.58622\tvalidation_1-mlogloss:0.74286\n[78]\tvalidation_0-mlogloss:0.58529\tvalidation_1-mlogloss:0.74248\n[79]\tvalidation_0-mlogloss:0.58415\tvalidation_1-mlogloss:0.74246\n[80]\tvalidation_0-mlogloss:0.58287\tvalidation_1-mlogloss:0.74214\n[81]\tvalidation_0-mlogloss:0.58004\tvalidation_1-mlogloss:0.74133\n[82]\tvalidation_0-mlogloss:0.57786\tvalidation_1-mlogloss:0.74070\n[83]\tvalidation_0-mlogloss:0.57591\tvalidation_1-mlogloss:0.74088\n[84]\tvalidation_0-mlogloss:0.57453\tvalidation_1-mlogloss:0.74086\n[85]\tvalidation_0-mlogloss:0.57373\tvalidation_1-mlogloss:0.74094\n[86]\tvalidation_0-mlogloss:0.57195\tvalidation_1-mlogloss:0.74059\n[87]\tvalidation_0-mlogloss:0.56968\tvalidation_1-mlogloss:0.73993\n[88]\tvalidation_0-mlogloss:0.56859\tvalidation_1-mlogloss:0.73970\n[89]\tvalidation_0-mlogloss:0.56711\tvalidation_1-mlogloss:0.73879\n[90]\tvalidation_0-mlogloss:0.56550\tvalidation_1-mlogloss:0.73842\n[91]\tvalidation_0-mlogloss:0.56286\tvalidation_1-mlogloss:0.73791\n[92]\tvalidation_0-mlogloss:0.56198\tvalidation_1-mlogloss:0.73775\n[93]\tvalidation_0-mlogloss:0.55847\tvalidation_1-mlogloss:0.73696\n[94]\tvalidation_0-mlogloss:0.55671\tvalidation_1-mlogloss:0.73633\n[95]\tvalidation_0-mlogloss:0.55546\tvalidation_1-mlogloss:0.73617\n[96]\tvalidation_0-mlogloss:0.55411\tvalidation_1-mlogloss:0.73574\n[97]\tvalidation_0-mlogloss:0.55297\tvalidation_1-mlogloss:0.73564\n[98]\tvalidation_0-mlogloss:0.55245\tvalidation_1-mlogloss:0.73552\n[99]\tvalidation_0-mlogloss:0.55159\tvalidation_1-mlogloss:0.73559\n[100]\tvalidation_0-mlogloss:0.55056\tvalidation_1-mlogloss:0.73555\n[101]\tvalidation_0-mlogloss:0.54852\tvalidation_1-mlogloss:0.73533\n[102]\tvalidation_0-mlogloss:0.54704\tvalidation_1-mlogloss:0.73542\n[103]\tvalidation_0-mlogloss:0.54597\tvalidation_1-mlogloss:0.73532\n[104]\tvalidation_0-mlogloss:0.54480\tvalidation_1-mlogloss:0.73459\n[105]\tvalidation_0-mlogloss:0.54287\tvalidation_1-mlogloss:0.73397\n[106]\tvalidation_0-mlogloss:0.54150\tvalidation_1-mlogloss:0.73355\n[107]\tvalidation_0-mlogloss:0.54043\tvalidation_1-mlogloss:0.73336\n[108]\tvalidation_0-mlogloss:0.53871\tvalidation_1-mlogloss:0.73318\n[109]\tvalidation_0-mlogloss:0.53680\tvalidation_1-mlogloss:0.73352\n[110]\tvalidation_0-mlogloss:0.53355\tvalidation_1-mlogloss:0.73243\n[111]\tvalidation_0-mlogloss:0.53242\tvalidation_1-mlogloss:0.73232\n[112]\tvalidation_0-mlogloss:0.53113\tvalidation_1-mlogloss:0.73220\n[113]\tvalidation_0-mlogloss:0.53021\tvalidation_1-mlogloss:0.73200\n[114]\tvalidation_0-mlogloss:0.52901\tvalidation_1-mlogloss:0.73171\n[115]\tvalidation_0-mlogloss:0.52730\tvalidation_1-mlogloss:0.73172\n[116]\tvalidation_0-mlogloss:0.52552\tvalidation_1-mlogloss:0.73152\n[117]\tvalidation_0-mlogloss:0.52403\tvalidation_1-mlogloss:0.73135\n[118]\tvalidation_0-mlogloss:0.52186\tvalidation_1-mlogloss:0.73092\n[119]\tvalidation_0-mlogloss:0.52076\tvalidation_1-mlogloss:0.73062\n[120]\tvalidation_0-mlogloss:0.51957\tvalidation_1-mlogloss:0.73071\n[121]\tvalidation_0-mlogloss:0.51711\tvalidation_1-mlogloss:0.73013\n[122]\tvalidation_0-mlogloss:0.51555\tvalidation_1-mlogloss:0.73001\n[123]\tvalidation_0-mlogloss:0.51446\tvalidation_1-mlogloss:0.73035\n[124]\tvalidation_0-mlogloss:0.51342\tvalidation_1-mlogloss:0.73031\n[125]\tvalidation_0-mlogloss:0.51187\tvalidation_1-mlogloss:0.73036\n[126]\tvalidation_0-mlogloss:0.51094\tvalidation_1-mlogloss:0.73056\n[127]\tvalidation_0-mlogloss:0.50971\tvalidation_1-mlogloss:0.73016\n[128]\tvalidation_0-mlogloss:0.50830\tvalidation_1-mlogloss:0.73003\n[129]\tvalidation_0-mlogloss:0.50616\tvalidation_1-mlogloss:0.72964\n[130]\tvalidation_0-mlogloss:0.50459\tvalidation_1-mlogloss:0.72959\n[131]\tvalidation_0-mlogloss:0.50339\tvalidation_1-mlogloss:0.72905\n[132]\tvalidation_0-mlogloss:0.50178\tvalidation_1-mlogloss:0.72909\n[133]\tvalidation_0-mlogloss:0.50081\tvalidation_1-mlogloss:0.72935\n[134]\tvalidation_0-mlogloss:0.49900\tvalidation_1-mlogloss:0.72873\n[135]\tvalidation_0-mlogloss:0.49759\tvalidation_1-mlogloss:0.72894\n[136]\tvalidation_0-mlogloss:0.49666\tvalidation_1-mlogloss:0.72868\n[137]\tvalidation_0-mlogloss:0.49561\tvalidation_1-mlogloss:0.72868\n[138]\tvalidation_0-mlogloss:0.49446\tvalidation_1-mlogloss:0.72899\n[139]\tvalidation_0-mlogloss:0.49239\tvalidation_1-mlogloss:0.72799\n[140]\tvalidation_0-mlogloss:0.49134\tvalidation_1-mlogloss:0.72795\n[141]\tvalidation_0-mlogloss:0.49004\tvalidation_1-mlogloss:0.72818\n[142]\tvalidation_0-mlogloss:0.48900\tvalidation_1-mlogloss:0.72807\n[143]\tvalidation_0-mlogloss:0.48769\tvalidation_1-mlogloss:0.72765\n[144]\tvalidation_0-mlogloss:0.48607\tvalidation_1-mlogloss:0.72786\n[145]\tvalidation_0-mlogloss:0.48434\tvalidation_1-mlogloss:0.72812\n[146]\tvalidation_0-mlogloss:0.48280\tvalidation_1-mlogloss:0.72801\n[147]\tvalidation_0-mlogloss:0.48158\tvalidation_1-mlogloss:0.72824\n[148]\tvalidation_0-mlogloss:0.48034\tvalidation_1-mlogloss:0.72786\n[149]\tvalidation_0-mlogloss:0.47901\tvalidation_1-mlogloss:0.72777\n[150]\tvalidation_0-mlogloss:0.47794\tvalidation_1-mlogloss:0.72830\n[151]\tvalidation_0-mlogloss:0.47684\tvalidation_1-mlogloss:0.72847\n[152]\tvalidation_0-mlogloss:0.47596\tvalidation_1-mlogloss:0.72853\n[153]\tvalidation_0-mlogloss:0.47560\tvalidation_1-mlogloss:0.72853\n","output_type":"stream"},{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n              min_child_weight=1, missing=nan, monotone_constraints='()',\n              n_estimators=1000, n_jobs=4, num_parallel_tree=1,\n              objective='multi:softprob', random_state=71, reg_alpha=0,\n              reg_lambda=1, scale_pos_weight=None, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)"},"metadata":{}}]},{"cell_type":"code","source":"xgb_pred=pd.DataFrame(xgb_model.predict_proba(testc))\nxgb_pred['index']=submit['index']\nxgb_pred=xgb_pred[['index',0,1,2]]\nxgb_pred.head()","metadata":{"trusted":true},"execution_count":66,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n  \"because it will generate extra copies and increase \" +\n","output_type":"stream"},{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"   index         0         1         2\n0  26457  0.105818  0.207735  0.686448\n1  26458  0.361630  0.214214  0.424156\n2  26459  0.051753  0.091378  0.856869\n3  26460  0.120335  0.097700  0.781965\n4  26461  0.121454  0.178253  0.700293","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>26457</td>\n      <td>0.105818</td>\n      <td>0.207735</td>\n      <td>0.686448</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26458</td>\n      <td>0.361630</td>\n      <td>0.214214</td>\n      <td>0.424156</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26459</td>\n      <td>0.051753</td>\n      <td>0.091378</td>\n      <td>0.856869</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>26460</td>\n      <td>0.120335</td>\n      <td>0.097700</td>\n      <td>0.781965</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>26461</td>\n      <td>0.121454</td>\n      <td>0.178253</td>\n      <td>0.700293</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# xgb.plot_importance(model,max_num_features=10)","metadata":{"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# xgb.to_graphviz(model)","metadata":{"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"# pred.to_csv('20210508_xgboost_all_cat_grpincome.csv',index=False)","metadata":{"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"## rf","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nparams={\n    'n_estimators':1000,\n    'random_state':71,\n    'criterion':'gini',\n    'verbose':1,\n    'class_weight':'balanced',\n    'n_jobs':-1,\n    'oob_score':True\n}\nrf_model=RandomForestClassifier(**params)\n\nrf_model.fit(X,y.values)","metadata":{"trusted":true},"execution_count":70,"outputs":[{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    1.3s\n[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    5.9s\n[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   13.2s\n[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:   23.5s\n[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   29.4s finished\n","output_type":"stream"},{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"RandomForestClassifier(class_weight='balanced', n_estimators=1000, n_jobs=-1,\n                       oob_score=True, random_state=71, verbose=1)"},"metadata":{}}]},{"cell_type":"code","source":"rf_pred=pd.DataFrame(rf_model.predict_proba(testc))\nrf_pred['index']=submit['index']\nrf_pred=rf_pred[['index',0,1,2]]\nrf_pred.head()\n# from sklearn.metrics import confusion_matrix\n\n# confusion_matrix(y_test,pred)\n\n# fi = pd.DataFrame({'feature': list(train.drop('credit',axis=1).columns),\n#                    'importance': model.feature_importances_}).\\\n#                     sort_values('importance', ascending = False)\n\n# fi.head()","metadata":{"trusted":true},"execution_count":71,"outputs":[{"name":"stderr","text":"[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.6s\n[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    1.0s\n[Parallel(n_jobs=4)]: Done 1000 out of 1000 | elapsed:    1.2s finished\n","output_type":"stream"},{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"   index         0         1         2\n0  26457  0.090496  0.159889  0.749615\n1  26458  0.583963  0.193000  0.223037\n2  26459  0.020318  0.034404  0.945279\n3  26460  0.028387  0.062197  0.909416\n4  26461  0.089419  0.183572  0.727009","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>26457</td>\n      <td>0.090496</td>\n      <td>0.159889</td>\n      <td>0.749615</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26458</td>\n      <td>0.583963</td>\n      <td>0.193000</td>\n      <td>0.223037</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26459</td>\n      <td>0.020318</td>\n      <td>0.034404</td>\n      <td>0.945279</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>26460</td>\n      <td>0.028387</td>\n      <td>0.062197</td>\n      <td>0.909416</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>26461</td>\n      <td>0.089419</td>\n      <td>0.183572</td>\n      <td>0.727009</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# rf_pred.to_csv('20210510_rf.csv',index=False)","metadata":{"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"## lgb","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\n\nparams={\n    'objective':'multiclass',\n    'random_state':71,\n    'n_estimators':1000\n}\nlgb_model=lgb.LGBMClassifier(**params)\n\nlgb_model.fit(X_train,y_train,\n              eval_set=[(X_train, y_train),(X_test,y_test)],\n              eval_metric='multi_logloss',verbose=True,\n              early_stopping_rounds=10)","metadata":{"scrolled":true,"trusted":true},"execution_count":73,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}},{"name":"stdout","text":"[1]\ttraining's multi_logloss: 0.862629\tvalid_1's multi_logloss: 0.863758\nTraining until validation scores don't improve for 10 rounds\n[2]\ttraining's multi_logloss: 0.847398\tvalid_1's multi_logloss: 0.850168\n[3]\ttraining's multi_logloss: 0.8357\tvalid_1's multi_logloss: 0.839879\n[4]\ttraining's multi_logloss: 0.825534\tvalid_1's multi_logloss: 0.831843\n[5]\ttraining's multi_logloss: 0.81736\tvalid_1's multi_logloss: 0.824967\n[6]\ttraining's multi_logloss: 0.809943\tvalid_1's multi_logloss: 0.81933\n[7]\ttraining's multi_logloss: 0.803022\tvalid_1's multi_logloss: 0.814355\n[8]\ttraining's multi_logloss: 0.797087\tvalid_1's multi_logloss: 0.810275\n[9]\ttraining's multi_logloss: 0.791884\tvalid_1's multi_logloss: 0.80691\n[10]\ttraining's multi_logloss: 0.787077\tvalid_1's multi_logloss: 0.803546\n[11]\ttraining's multi_logloss: 0.782427\tvalid_1's multi_logloss: 0.800358\n[12]\ttraining's multi_logloss: 0.778234\tvalid_1's multi_logloss: 0.797693\n[13]\ttraining's multi_logloss: 0.774504\tvalid_1's multi_logloss: 0.79549\n[14]\ttraining's multi_logloss: 0.770814\tvalid_1's multi_logloss: 0.793786\n[15]\ttraining's multi_logloss: 0.767379\tvalid_1's multi_logloss: 0.791902\n[16]\ttraining's multi_logloss: 0.764262\tvalid_1's multi_logloss: 0.790516\n[17]\ttraining's multi_logloss: 0.761332\tvalid_1's multi_logloss: 0.788789\n[18]\ttraining's multi_logloss: 0.758688\tvalid_1's multi_logloss: 0.787412\n[19]\ttraining's multi_logloss: 0.755972\tvalid_1's multi_logloss: 0.786157\n[20]\ttraining's multi_logloss: 0.753233\tvalid_1's multi_logloss: 0.784815\n[21]\ttraining's multi_logloss: 0.750778\tvalid_1's multi_logloss: 0.783869\n[22]\ttraining's multi_logloss: 0.748417\tvalid_1's multi_logloss: 0.78238\n[23]\ttraining's multi_logloss: 0.745829\tvalid_1's multi_logloss: 0.781147\n[24]\ttraining's multi_logloss: 0.743184\tvalid_1's multi_logloss: 0.780033\n[25]\ttraining's multi_logloss: 0.740779\tvalid_1's multi_logloss: 0.778907\n[26]\ttraining's multi_logloss: 0.738012\tvalid_1's multi_logloss: 0.777861\n[27]\ttraining's multi_logloss: 0.735732\tvalid_1's multi_logloss: 0.77734\n[28]\ttraining's multi_logloss: 0.733466\tvalid_1's multi_logloss: 0.776743\n[29]\ttraining's multi_logloss: 0.731163\tvalid_1's multi_logloss: 0.776099\n[30]\ttraining's multi_logloss: 0.72936\tvalid_1's multi_logloss: 0.775616\n[31]\ttraining's multi_logloss: 0.726729\tvalid_1's multi_logloss: 0.774465\n[32]\ttraining's multi_logloss: 0.724765\tvalid_1's multi_logloss: 0.773459\n[33]\ttraining's multi_logloss: 0.722865\tvalid_1's multi_logloss: 0.773019\n[34]\ttraining's multi_logloss: 0.721013\tvalid_1's multi_logloss: 0.772198\n[35]\ttraining's multi_logloss: 0.719034\tvalid_1's multi_logloss: 0.771696\n[36]\ttraining's multi_logloss: 0.717237\tvalid_1's multi_logloss: 0.770986\n[37]\ttraining's multi_logloss: 0.715141\tvalid_1's multi_logloss: 0.770102\n[38]\ttraining's multi_logloss: 0.713481\tvalid_1's multi_logloss: 0.769705\n[39]\ttraining's multi_logloss: 0.711354\tvalid_1's multi_logloss: 0.769141\n[40]\ttraining's multi_logloss: 0.709311\tvalid_1's multi_logloss: 0.767874\n[41]\ttraining's multi_logloss: 0.707469\tvalid_1's multi_logloss: 0.767068\n[42]\ttraining's multi_logloss: 0.705532\tvalid_1's multi_logloss: 0.766603\n[43]\ttraining's multi_logloss: 0.704029\tvalid_1's multi_logloss: 0.766319\n[44]\ttraining's multi_logloss: 0.702074\tvalid_1's multi_logloss: 0.765431\n[45]\ttraining's multi_logloss: 0.700075\tvalid_1's multi_logloss: 0.764815\n[46]\ttraining's multi_logloss: 0.69807\tvalid_1's multi_logloss: 0.764201\n[47]\ttraining's multi_logloss: 0.696324\tvalid_1's multi_logloss: 0.763426\n[48]\ttraining's multi_logloss: 0.694901\tvalid_1's multi_logloss: 0.76267\n[49]\ttraining's multi_logloss: 0.69344\tvalid_1's multi_logloss: 0.762062\n[50]\ttraining's multi_logloss: 0.691677\tvalid_1's multi_logloss: 0.761317\n[51]\ttraining's multi_logloss: 0.689929\tvalid_1's multi_logloss: 0.760933\n[52]\ttraining's multi_logloss: 0.688397\tvalid_1's multi_logloss: 0.760665\n[53]\ttraining's multi_logloss: 0.68681\tvalid_1's multi_logloss: 0.760328\n[54]\ttraining's multi_logloss: 0.684997\tvalid_1's multi_logloss: 0.759673\n[55]\ttraining's multi_logloss: 0.683798\tvalid_1's multi_logloss: 0.759448\n[56]\ttraining's multi_logloss: 0.682339\tvalid_1's multi_logloss: 0.759144\n[57]\ttraining's multi_logloss: 0.681004\tvalid_1's multi_logloss: 0.758763\n[58]\ttraining's multi_logloss: 0.679179\tvalid_1's multi_logloss: 0.758048\n[59]\ttraining's multi_logloss: 0.677629\tvalid_1's multi_logloss: 0.757856\n[60]\ttraining's multi_logloss: 0.6763\tvalid_1's multi_logloss: 0.757703\n[61]\ttraining's multi_logloss: 0.674838\tvalid_1's multi_logloss: 0.757052\n[62]\ttraining's multi_logloss: 0.673607\tvalid_1's multi_logloss: 0.756698\n[63]\ttraining's multi_logloss: 0.67201\tvalid_1's multi_logloss: 0.75618\n[64]\ttraining's multi_logloss: 0.670511\tvalid_1's multi_logloss: 0.755838\n[65]\ttraining's multi_logloss: 0.668886\tvalid_1's multi_logloss: 0.755283\n[66]\ttraining's multi_logloss: 0.667466\tvalid_1's multi_logloss: 0.754854\n[67]\ttraining's multi_logloss: 0.666219\tvalid_1's multi_logloss: 0.754409\n[68]\ttraining's multi_logloss: 0.665027\tvalid_1's multi_logloss: 0.75404\n[69]\ttraining's multi_logloss: 0.663933\tvalid_1's multi_logloss: 0.753654\n[70]\ttraining's multi_logloss: 0.662538\tvalid_1's multi_logloss: 0.753462\n[71]\ttraining's multi_logloss: 0.661111\tvalid_1's multi_logloss: 0.752899\n[72]\ttraining's multi_logloss: 0.659902\tvalid_1's multi_logloss: 0.752652\n[73]\ttraining's multi_logloss: 0.65829\tvalid_1's multi_logloss: 0.75209\n[74]\ttraining's multi_logloss: 0.656965\tvalid_1's multi_logloss: 0.751604\n[75]\ttraining's multi_logloss: 0.655621\tvalid_1's multi_logloss: 0.751488\n[76]\ttraining's multi_logloss: 0.654582\tvalid_1's multi_logloss: 0.751395\n[77]\ttraining's multi_logloss: 0.653247\tvalid_1's multi_logloss: 0.751062\n[78]\ttraining's multi_logloss: 0.651845\tvalid_1's multi_logloss: 0.750648\n[79]\ttraining's multi_logloss: 0.650647\tvalid_1's multi_logloss: 0.750268\n[80]\ttraining's multi_logloss: 0.649665\tvalid_1's multi_logloss: 0.750063\n[81]\ttraining's multi_logloss: 0.648476\tvalid_1's multi_logloss: 0.749588\n[82]\ttraining's multi_logloss: 0.64682\tvalid_1's multi_logloss: 0.748735\n[83]\ttraining's multi_logloss: 0.645708\tvalid_1's multi_logloss: 0.748711\n[84]\ttraining's multi_logloss: 0.644257\tvalid_1's multi_logloss: 0.748063\n[85]\ttraining's multi_logloss: 0.643263\tvalid_1's multi_logloss: 0.748043\n[86]\ttraining's multi_logloss: 0.642286\tvalid_1's multi_logloss: 0.74808\n[87]\ttraining's multi_logloss: 0.640721\tvalid_1's multi_logloss: 0.747486\n[88]\ttraining's multi_logloss: 0.639212\tvalid_1's multi_logloss: 0.74698\n[89]\ttraining's multi_logloss: 0.638126\tvalid_1's multi_logloss: 0.746487\n[90]\ttraining's multi_logloss: 0.63664\tvalid_1's multi_logloss: 0.745939\n[91]\ttraining's multi_logloss: 0.635078\tvalid_1's multi_logloss: 0.745908\n[92]\ttraining's multi_logloss: 0.634063\tvalid_1's multi_logloss: 0.745988\n[93]\ttraining's multi_logloss: 0.632831\tvalid_1's multi_logloss: 0.745555\n[94]\ttraining's multi_logloss: 0.631839\tvalid_1's multi_logloss: 0.745266\n[95]\ttraining's multi_logloss: 0.630228\tvalid_1's multi_logloss: 0.745\n[96]\ttraining's multi_logloss: 0.628959\tvalid_1's multi_logloss: 0.744571\n[97]\ttraining's multi_logloss: 0.627685\tvalid_1's multi_logloss: 0.744576\n[98]\ttraining's multi_logloss: 0.626243\tvalid_1's multi_logloss: 0.744246\n[99]\ttraining's multi_logloss: 0.625323\tvalid_1's multi_logloss: 0.744255\n[100]\ttraining's multi_logloss: 0.624137\tvalid_1's multi_logloss: 0.744044\n[101]\ttraining's multi_logloss: 0.62307\tvalid_1's multi_logloss: 0.743719\n[102]\ttraining's multi_logloss: 0.622146\tvalid_1's multi_logloss: 0.743514\n[103]\ttraining's multi_logloss: 0.621225\tvalid_1's multi_logloss: 0.74329\n[104]\ttraining's multi_logloss: 0.620154\tvalid_1's multi_logloss: 0.742996\n[105]\ttraining's multi_logloss: 0.61912\tvalid_1's multi_logloss: 0.742724\n[106]\ttraining's multi_logloss: 0.618216\tvalid_1's multi_logloss: 0.742625\n[107]\ttraining's multi_logloss: 0.616862\tvalid_1's multi_logloss: 0.742171\n[108]\ttraining's multi_logloss: 0.615571\tvalid_1's multi_logloss: 0.741944\n[109]\ttraining's multi_logloss: 0.614479\tvalid_1's multi_logloss: 0.742108\n[110]\ttraining's multi_logloss: 0.613346\tvalid_1's multi_logloss: 0.741582\n[111]\ttraining's multi_logloss: 0.61257\tvalid_1's multi_logloss: 0.741485\n[112]\ttraining's multi_logloss: 0.61149\tvalid_1's multi_logloss: 0.741237\n[113]\ttraining's multi_logloss: 0.610248\tvalid_1's multi_logloss: 0.740939\n[114]\ttraining's multi_logloss: 0.609147\tvalid_1's multi_logloss: 0.741085\n[115]\ttraining's multi_logloss: 0.608279\tvalid_1's multi_logloss: 0.741003\n[116]\ttraining's multi_logloss: 0.607383\tvalid_1's multi_logloss: 0.741121\n[117]\ttraining's multi_logloss: 0.606308\tvalid_1's multi_logloss: 0.740874\n[118]\ttraining's multi_logloss: 0.605288\tvalid_1's multi_logloss: 0.740856\n[119]\ttraining's multi_logloss: 0.60437\tvalid_1's multi_logloss: 0.740661\n[120]\ttraining's multi_logloss: 0.603369\tvalid_1's multi_logloss: 0.74062\n[121]\ttraining's multi_logloss: 0.602195\tvalid_1's multi_logloss: 0.739967\n[122]\ttraining's multi_logloss: 0.601175\tvalid_1's multi_logloss: 0.739929\n[123]\ttraining's multi_logloss: 0.600264\tvalid_1's multi_logloss: 0.739547\n[124]\ttraining's multi_logloss: 0.599229\tvalid_1's multi_logloss: 0.739443\n[125]\ttraining's multi_logloss: 0.598075\tvalid_1's multi_logloss: 0.739197\n[126]\ttraining's multi_logloss: 0.597232\tvalid_1's multi_logloss: 0.739081\n[127]\ttraining's multi_logloss: 0.595951\tvalid_1's multi_logloss: 0.738749\n[128]\ttraining's multi_logloss: 0.59495\tvalid_1's multi_logloss: 0.738954\n[129]\ttraining's multi_logloss: 0.593798\tvalid_1's multi_logloss: 0.738454\n[130]\ttraining's multi_logloss: 0.592607\tvalid_1's multi_logloss: 0.738424\n[131]\ttraining's multi_logloss: 0.591558\tvalid_1's multi_logloss: 0.738177\n[132]\ttraining's multi_logloss: 0.5907\tvalid_1's multi_logloss: 0.737913\n[133]\ttraining's multi_logloss: 0.589885\tvalid_1's multi_logloss: 0.737672\n[134]\ttraining's multi_logloss: 0.588895\tvalid_1's multi_logloss: 0.737448\n[135]\ttraining's multi_logloss: 0.588091\tvalid_1's multi_logloss: 0.736951\n[136]\ttraining's multi_logloss: 0.587208\tvalid_1's multi_logloss: 0.736744\n[137]\ttraining's multi_logloss: 0.586176\tvalid_1's multi_logloss: 0.736385\n[138]\ttraining's multi_logloss: 0.585411\tvalid_1's multi_logloss: 0.736151\n[139]\ttraining's multi_logloss: 0.584214\tvalid_1's multi_logloss: 0.735885\n[140]\ttraining's multi_logloss: 0.583422\tvalid_1's multi_logloss: 0.735734\n[141]\ttraining's multi_logloss: 0.582232\tvalid_1's multi_logloss: 0.735417\n[142]\ttraining's multi_logloss: 0.581472\tvalid_1's multi_logloss: 0.7355\n[143]\ttraining's multi_logloss: 0.580398\tvalid_1's multi_logloss: 0.735385\n[144]\ttraining's multi_logloss: 0.579427\tvalid_1's multi_logloss: 0.735243\n[145]\ttraining's multi_logloss: 0.578515\tvalid_1's multi_logloss: 0.73507\n[146]\ttraining's multi_logloss: 0.57721\tvalid_1's multi_logloss: 0.734605\n[147]\ttraining's multi_logloss: 0.576259\tvalid_1's multi_logloss: 0.734329\n[148]\ttraining's multi_logloss: 0.574938\tvalid_1's multi_logloss: 0.733876\n[149]\ttraining's multi_logloss: 0.573981\tvalid_1's multi_logloss: 0.73401\n[150]\ttraining's multi_logloss: 0.573027\tvalid_1's multi_logloss: 0.733709\n[151]\ttraining's multi_logloss: 0.572313\tvalid_1's multi_logloss: 0.733664\n[152]\ttraining's multi_logloss: 0.571514\tvalid_1's multi_logloss: 0.733505\n[153]\ttraining's multi_logloss: 0.570474\tvalid_1's multi_logloss: 0.733273\n[154]\ttraining's multi_logloss: 0.569513\tvalid_1's multi_logloss: 0.73288\n[155]\ttraining's multi_logloss: 0.568286\tvalid_1's multi_logloss: 0.73248\n[156]\ttraining's multi_logloss: 0.567179\tvalid_1's multi_logloss: 0.73222\n[157]\ttraining's multi_logloss: 0.566043\tvalid_1's multi_logloss: 0.731848\n[158]\ttraining's multi_logloss: 0.565183\tvalid_1's multi_logloss: 0.731673\n[159]\ttraining's multi_logloss: 0.564274\tvalid_1's multi_logloss: 0.731554\n[160]\ttraining's multi_logloss: 0.563466\tvalid_1's multi_logloss: 0.731518\n[161]\ttraining's multi_logloss: 0.562516\tvalid_1's multi_logloss: 0.731369\n[162]\ttraining's multi_logloss: 0.561593\tvalid_1's multi_logloss: 0.730851\n[163]\ttraining's multi_logloss: 0.560995\tvalid_1's multi_logloss: 0.730882\n[164]\ttraining's multi_logloss: 0.560011\tvalid_1's multi_logloss: 0.731104\n[165]\ttraining's multi_logloss: 0.559147\tvalid_1's multi_logloss: 0.731081\n[166]\ttraining's multi_logloss: 0.558278\tvalid_1's multi_logloss: 0.730822\n[167]\ttraining's multi_logloss: 0.557518\tvalid_1's multi_logloss: 0.730829\n[168]\ttraining's multi_logloss: 0.556607\tvalid_1's multi_logloss: 0.730337\n[169]\ttraining's multi_logloss: 0.555712\tvalid_1's multi_logloss: 0.730174\n[170]\ttraining's multi_logloss: 0.554609\tvalid_1's multi_logloss: 0.729882\n[171]\ttraining's multi_logloss: 0.553621\tvalid_1's multi_logloss: 0.729751\n[172]\ttraining's multi_logloss: 0.552618\tvalid_1's multi_logloss: 0.729456\n[173]\ttraining's multi_logloss: 0.5517\tvalid_1's multi_logloss: 0.729362\n[174]\ttraining's multi_logloss: 0.550918\tvalid_1's multi_logloss: 0.729445\n[175]\ttraining's multi_logloss: 0.550074\tvalid_1's multi_logloss: 0.72938\n[176]\ttraining's multi_logloss: 0.549371\tvalid_1's multi_logloss: 0.729375\n[177]\ttraining's multi_logloss: 0.548639\tvalid_1's multi_logloss: 0.729231\n[178]\ttraining's multi_logloss: 0.547963\tvalid_1's multi_logloss: 0.72902\n[179]\ttraining's multi_logloss: 0.547049\tvalid_1's multi_logloss: 0.728861\n[180]\ttraining's multi_logloss: 0.546221\tvalid_1's multi_logloss: 0.728561\n[181]\ttraining's multi_logloss: 0.545202\tvalid_1's multi_logloss: 0.728212\n[182]\ttraining's multi_logloss: 0.544232\tvalid_1's multi_logloss: 0.727929\n[183]\ttraining's multi_logloss: 0.543577\tvalid_1's multi_logloss: 0.727681\n[184]\ttraining's multi_logloss: 0.542686\tvalid_1's multi_logloss: 0.727573\n[185]\ttraining's multi_logloss: 0.541833\tvalid_1's multi_logloss: 0.727314\n[186]\ttraining's multi_logloss: 0.540969\tvalid_1's multi_logloss: 0.727574\n[187]\ttraining's multi_logloss: 0.540202\tvalid_1's multi_logloss: 0.727422\n[188]\ttraining's multi_logloss: 0.539207\tvalid_1's multi_logloss: 0.727222\n[189]\ttraining's multi_logloss: 0.53833\tvalid_1's multi_logloss: 0.727103\n[190]\ttraining's multi_logloss: 0.537449\tvalid_1's multi_logloss: 0.726851\n[191]\ttraining's multi_logloss: 0.536547\tvalid_1's multi_logloss: 0.726643\n[192]\ttraining's multi_logloss: 0.535765\tvalid_1's multi_logloss: 0.726485\n[193]\ttraining's multi_logloss: 0.535051\tvalid_1's multi_logloss: 0.726072\n[194]\ttraining's multi_logloss: 0.534212\tvalid_1's multi_logloss: 0.726033\n[195]\ttraining's multi_logloss: 0.533115\tvalid_1's multi_logloss: 0.725749\n[196]\ttraining's multi_logloss: 0.532496\tvalid_1's multi_logloss: 0.725571\n[197]\ttraining's multi_logloss: 0.531709\tvalid_1's multi_logloss: 0.725575\n[198]\ttraining's multi_logloss: 0.530879\tvalid_1's multi_logloss: 0.725124\n[199]\ttraining's multi_logloss: 0.530065\tvalid_1's multi_logloss: 0.725317\n[200]\ttraining's multi_logloss: 0.529398\tvalid_1's multi_logloss: 0.725296\n[201]\ttraining's multi_logloss: 0.528582\tvalid_1's multi_logloss: 0.725343\n[202]\ttraining's multi_logloss: 0.527892\tvalid_1's multi_logloss: 0.725229\n[203]\ttraining's multi_logloss: 0.527081\tvalid_1's multi_logloss: 0.725244\n[204]\ttraining's multi_logloss: 0.526306\tvalid_1's multi_logloss: 0.725066\n[205]\ttraining's multi_logloss: 0.525636\tvalid_1's multi_logloss: 0.724799\n[206]\ttraining's multi_logloss: 0.524865\tvalid_1's multi_logloss: 0.724572\n[207]\ttraining's multi_logloss: 0.523986\tvalid_1's multi_logloss: 0.724359\n[208]\ttraining's multi_logloss: 0.523076\tvalid_1's multi_logloss: 0.724195\n[209]\ttraining's multi_logloss: 0.522401\tvalid_1's multi_logloss: 0.724244\n[210]\ttraining's multi_logloss: 0.52166\tvalid_1's multi_logloss: 0.724059\n[211]\ttraining's multi_logloss: 0.521067\tvalid_1's multi_logloss: 0.723881\n[212]\ttraining's multi_logloss: 0.520022\tvalid_1's multi_logloss: 0.723582\n[213]\ttraining's multi_logloss: 0.519348\tvalid_1's multi_logloss: 0.723358\n[214]\ttraining's multi_logloss: 0.518762\tvalid_1's multi_logloss: 0.723155\n[215]\ttraining's multi_logloss: 0.518041\tvalid_1's multi_logloss: 0.72324\n[216]\ttraining's multi_logloss: 0.517405\tvalid_1's multi_logloss: 0.72312\n[217]\ttraining's multi_logloss: 0.516663\tvalid_1's multi_logloss: 0.722905\n[218]\ttraining's multi_logloss: 0.516018\tvalid_1's multi_logloss: 0.722768\n[219]\ttraining's multi_logloss: 0.515186\tvalid_1's multi_logloss: 0.722708\n[220]\ttraining's multi_logloss: 0.514358\tvalid_1's multi_logloss: 0.722534\n[221]\ttraining's multi_logloss: 0.513681\tvalid_1's multi_logloss: 0.722405\n[222]\ttraining's multi_logloss: 0.51276\tvalid_1's multi_logloss: 0.722336\n[223]\ttraining's multi_logloss: 0.511952\tvalid_1's multi_logloss: 0.722259\n[224]\ttraining's multi_logloss: 0.511406\tvalid_1's multi_logloss: 0.72227\n[225]\ttraining's multi_logloss: 0.510658\tvalid_1's multi_logloss: 0.722174\n[226]\ttraining's multi_logloss: 0.51\tvalid_1's multi_logloss: 0.722156\n[227]\ttraining's multi_logloss: 0.509411\tvalid_1's multi_logloss: 0.722346\n[228]\ttraining's multi_logloss: 0.508854\tvalid_1's multi_logloss: 0.722339\n[229]\ttraining's multi_logloss: 0.508241\tvalid_1's multi_logloss: 0.722517\n[230]\ttraining's multi_logloss: 0.50756\tvalid_1's multi_logloss: 0.72209\n[231]\ttraining's multi_logloss: 0.506985\tvalid_1's multi_logloss: 0.722034\n[232]\ttraining's multi_logloss: 0.506468\tvalid_1's multi_logloss: 0.72189\n[233]\ttraining's multi_logloss: 0.505635\tvalid_1's multi_logloss: 0.721842\n[234]\ttraining's multi_logloss: 0.504903\tvalid_1's multi_logloss: 0.721625\n[235]\ttraining's multi_logloss: 0.504167\tvalid_1's multi_logloss: 0.721308\n[236]\ttraining's multi_logloss: 0.50362\tvalid_1's multi_logloss: 0.721488\n[237]\ttraining's multi_logloss: 0.502913\tvalid_1's multi_logloss: 0.721355\n[238]\ttraining's multi_logloss: 0.502123\tvalid_1's multi_logloss: 0.721363\n[239]\ttraining's multi_logloss: 0.501476\tvalid_1's multi_logloss: 0.72135\n[240]\ttraining's multi_logloss: 0.50058\tvalid_1's multi_logloss: 0.721241\n[241]\ttraining's multi_logloss: 0.499783\tvalid_1's multi_logloss: 0.721124\n[242]\ttraining's multi_logloss: 0.499004\tvalid_1's multi_logloss: 0.721233\n[243]\ttraining's multi_logloss: 0.498203\tvalid_1's multi_logloss: 0.721252\n[244]\ttraining's multi_logloss: 0.497462\tvalid_1's multi_logloss: 0.721278\n[245]\ttraining's multi_logloss: 0.496592\tvalid_1's multi_logloss: 0.721397\n[246]\ttraining's multi_logloss: 0.495794\tvalid_1's multi_logloss: 0.721163\n[247]\ttraining's multi_logloss: 0.495149\tvalid_1's multi_logloss: 0.72114\n[248]\ttraining's multi_logloss: 0.494392\tvalid_1's multi_logloss: 0.720998\n[249]\ttraining's multi_logloss: 0.493963\tvalid_1's multi_logloss: 0.721135\n[250]\ttraining's multi_logloss: 0.493292\tvalid_1's multi_logloss: 0.721069\n[251]\ttraining's multi_logloss: 0.492667\tvalid_1's multi_logloss: 0.721029\n[252]\ttraining's multi_logloss: 0.492009\tvalid_1's multi_logloss: 0.721226\n[253]\ttraining's multi_logloss: 0.49136\tvalid_1's multi_logloss: 0.720966\n[254]\ttraining's multi_logloss: 0.490596\tvalid_1's multi_logloss: 0.720997\n[255]\ttraining's multi_logloss: 0.489887\tvalid_1's multi_logloss: 0.721153\n[256]\ttraining's multi_logloss: 0.48923\tvalid_1's multi_logloss: 0.721371\n[257]\ttraining's multi_logloss: 0.488659\tvalid_1's multi_logloss: 0.721331\n[258]\ttraining's multi_logloss: 0.488019\tvalid_1's multi_logloss: 0.721363\n[259]\ttraining's multi_logloss: 0.487373\tvalid_1's multi_logloss: 0.721209\n[260]\ttraining's multi_logloss: 0.486604\tvalid_1's multi_logloss: 0.721198\n[261]\ttraining's multi_logloss: 0.486007\tvalid_1's multi_logloss: 0.72104\n[262]\ttraining's multi_logloss: 0.485372\tvalid_1's multi_logloss: 0.721063\n[263]\ttraining's multi_logloss: 0.484697\tvalid_1's multi_logloss: 0.72086\n[264]\ttraining's multi_logloss: 0.484134\tvalid_1's multi_logloss: 0.721168\n[265]\ttraining's multi_logloss: 0.483519\tvalid_1's multi_logloss: 0.72091\n[266]\ttraining's multi_logloss: 0.482697\tvalid_1's multi_logloss: 0.72075\n[267]\ttraining's multi_logloss: 0.481956\tvalid_1's multi_logloss: 0.720764\n[268]\ttraining's multi_logloss: 0.48114\tvalid_1's multi_logloss: 0.720746\n[269]\ttraining's multi_logloss: 0.480483\tvalid_1's multi_logloss: 0.720648\n[270]\ttraining's multi_logloss: 0.479739\tvalid_1's multi_logloss: 0.720573\n[271]\ttraining's multi_logloss: 0.479111\tvalid_1's multi_logloss: 0.720672\n[272]\ttraining's multi_logloss: 0.4784\tvalid_1's multi_logloss: 0.720485\n[273]\ttraining's multi_logloss: 0.477666\tvalid_1's multi_logloss: 0.720399\n[274]\ttraining's multi_logloss: 0.477145\tvalid_1's multi_logloss: 0.720411\n[275]\ttraining's multi_logloss: 0.476604\tvalid_1's multi_logloss: 0.720444\n[276]\ttraining's multi_logloss: 0.476002\tvalid_1's multi_logloss: 0.720385\n[277]\ttraining's multi_logloss: 0.475321\tvalid_1's multi_logloss: 0.720178\n[278]\ttraining's multi_logloss: 0.474763\tvalid_1's multi_logloss: 0.720335\n[279]\ttraining's multi_logloss: 0.474135\tvalid_1's multi_logloss: 0.720281\n[280]\ttraining's multi_logloss: 0.473603\tvalid_1's multi_logloss: 0.720242\n[281]\ttraining's multi_logloss: 0.47293\tvalid_1's multi_logloss: 0.720454\n[282]\ttraining's multi_logloss: 0.472389\tvalid_1's multi_logloss: 0.720505\n[283]\ttraining's multi_logloss: 0.471655\tvalid_1's multi_logloss: 0.72028\n[284]\ttraining's multi_logloss: 0.470938\tvalid_1's multi_logloss: 0.72046\n[285]\ttraining's multi_logloss: 0.47021\tvalid_1's multi_logloss: 0.720476\n[286]\ttraining's multi_logloss: 0.469559\tvalid_1's multi_logloss: 0.720382\n[287]\ttraining's multi_logloss: 0.468946\tvalid_1's multi_logloss: 0.720163\n[288]\ttraining's multi_logloss: 0.468448\tvalid_1's multi_logloss: 0.720219\n[289]\ttraining's multi_logloss: 0.467887\tvalid_1's multi_logloss: 0.720152\n[290]\ttraining's multi_logloss: 0.467326\tvalid_1's multi_logloss: 0.72016\n[291]\ttraining's multi_logloss: 0.466626\tvalid_1's multi_logloss: 0.72015\n[292]\ttraining's multi_logloss: 0.466122\tvalid_1's multi_logloss: 0.720263\n[293]\ttraining's multi_logloss: 0.465565\tvalid_1's multi_logloss: 0.720216\n[294]\ttraining's multi_logloss: 0.465174\tvalid_1's multi_logloss: 0.720256\n[295]\ttraining's multi_logloss: 0.4648\tvalid_1's multi_logloss: 0.720378\n[296]\ttraining's multi_logloss: 0.464377\tvalid_1's multi_logloss: 0.720449\n[297]\ttraining's multi_logloss: 0.463703\tvalid_1's multi_logloss: 0.720446\n[298]\ttraining's multi_logloss: 0.463213\tvalid_1's multi_logloss: 0.720729\n[299]\ttraining's multi_logloss: 0.46266\tvalid_1's multi_logloss: 0.720752\n[300]\ttraining's multi_logloss: 0.462087\tvalid_1's multi_logloss: 0.720632\n[301]\ttraining's multi_logloss: 0.461586\tvalid_1's multi_logloss: 0.720621\nEarly stopping, best iteration is:\n[291]\ttraining's multi_logloss: 0.466626\tvalid_1's multi_logloss: 0.72015\n","output_type":"stream"},{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"LGBMClassifier(n_estimators=1000, objective='multiclass', random_state=71)"},"metadata":{}}]},{"cell_type":"code","source":"lgb_pred=pd.DataFrame(lgb_model.predict_proba(testc))\nlgb_pred['index']=submit['index']\nlgb_pred=lgb_pred[['index',0,1,2]]\nlgb_pred.head()","metadata":{"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"   index         0         1         2\n0  26457  0.088507  0.289620  0.621872\n1  26458  0.321531  0.171406  0.507063\n2  26459  0.047231  0.061340  0.891430\n3  26460  0.057855  0.114074  0.828071\n4  26461  0.025804  0.088684  0.885512","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>26457</td>\n      <td>0.088507</td>\n      <td>0.289620</td>\n      <td>0.621872</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26458</td>\n      <td>0.321531</td>\n      <td>0.171406</td>\n      <td>0.507063</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26459</td>\n      <td>0.047231</td>\n      <td>0.061340</td>\n      <td>0.891430</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>26460</td>\n      <td>0.057855</td>\n      <td>0.114074</td>\n      <td>0.828071</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>26461</td>\n      <td>0.025804</td>\n      <td>0.088684</td>\n      <td>0.885512</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## svm","metadata":{}},{"cell_type":"code","source":"X=train.drop('credit',axis=1)\ny=train['credit']\n\nfrom sklearn.preprocessing import StandardScaler\n\nsd=StandardScaler()\n\nX=sd.fit_transform(X)\ntestc=test.copy()\ntestc=sd.transform(testc)","metadata":{"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\n\nparams={\n    'gamma':'auto',\n    'probability':True,\n    'random_state':71,\n#     'max_iter':300,\n    'class_weight':'balanced',\n    'kernel':'poly',\n    'shrinking':True,\n    'verbose':True\n}\n\nsvc_model=SVC(**params)\nsvc_model.fit(X,y.values)","metadata":{"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"[LibSVM]","output_type":"stream"},{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"SVC(class_weight='balanced', gamma='auto', kernel='poly', probability=True,\n    random_state=71, verbose=True)"},"metadata":{}}]},{"cell_type":"code","source":"svc_pred=pd.DataFrame(svc_model.predict_proba(testc))\nsvc_pred['index']=submit['index']\nsvc_pred=svc_pred[['index',0,1,2]]\nsvc_pred.head()","metadata":{"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"   index         0         1         2\n0  26457  0.086262  0.086680  0.827058\n1  26458  0.192828  0.132606  0.674567\n2  26459  0.127024  0.192656  0.680320\n3  26460  0.107909  0.127172  0.764919\n4  26461  0.093302  0.314897  0.591801","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>26457</td>\n      <td>0.086262</td>\n      <td>0.086680</td>\n      <td>0.827058</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26458</td>\n      <td>0.192828</td>\n      <td>0.132606</td>\n      <td>0.674567</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26459</td>\n      <td>0.127024</td>\n      <td>0.192656</td>\n      <td>0.680320</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>26460</td>\n      <td>0.107909</td>\n      <td>0.127172</td>\n      <td>0.764919</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>26461</td>\n      <td>0.093302</td>\n      <td>0.314897</td>\n      <td>0.591801</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## all","metadata":{}},{"cell_type":"code","source":"submit[['0','1','2']]=(xgb_pred[[0,1,2]]*rf_pred[[0,1,2]]*lgb_pred[[0,1,2]]*svc_pred[[0,1,2]])**(1/4)\nsubmit.head()","metadata":{"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"   index         0         1         2\n0  26457  0.092469  0.169929  0.717250\n1  26458  0.338268  0.175085  0.424128\n2  26459  0.050116  0.078071  0.837180\n3  26460  0.067956  0.096897  0.819235\n4  26461  0.071508  0.173866  0.718700","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>26457</td>\n      <td>0.092469</td>\n      <td>0.169929</td>\n      <td>0.717250</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26458</td>\n      <td>0.338268</td>\n      <td>0.175085</td>\n      <td>0.424128</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26459</td>\n      <td>0.050116</td>\n      <td>0.078071</td>\n      <td>0.837180</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>26460</td>\n      <td>0.067956</td>\n      <td>0.096897</td>\n      <td>0.819235</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>26461</td>\n      <td>0.071508</td>\n      <td>0.173866</td>\n      <td>0.718700</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"submit.to_csv('20210511_rf*xgb*lgb*svc_occpy+count.csv',index=False)","metadata":{"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"## income_total 정규성 x -> log 변환 한것을 파생변수로.\n## 다른건 몰라도 income_type이 pensioner인 사람들이 DAYS_BIRTH가 오래됐다. \n## -> DAYS_BIRTH는 -7500정도에서 -25000정도 사이 나이로 바꿀 필요가 있을까? -> 보류\n## edu_type에 따라 income_total과 DAYS_BIRTH는 선형관계가 있다. \n## DAYS_EMPLOYED 양수 값은 고용되지 않은 상태를 의미함 -> 제거 \n## family type과 성별에 따라 income_total의 차이가 없다. \n## FLAG_MOBIL 1밖에 없다. -> 제거 \n## 중복값 제거\n## family_size child_num 상관관계 -> 제거,income_total income_total_log 제거 그 외 그냥 둔다.\n\nsns.jointplot(data=train, x=\"family_size\", y=\"begin_month\", hue=\"house_type\")\n# sns.displot(data=train, x=\"income_total\", col=\"DAYS_BIRTH\", kde=True)\n# sns.relplot(data=train,x='income_total',y='DAYS_BIRTH',hue='edu_tpe')","metadata":{"_kg_hide-output":true,"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 연속형 - 연속\nsns.relplot(\n    data=train,\n    x=\"income_total\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\n\nsns.lmplot(data=tips, x=\"total_bill\", y=\"tip\", col=\"time\", hue=\"smoker\")\nsns.displot(data=tips, x=\"total_bill\", col=\"time\", kde=True)\nsns.jointplot(data=penguins, x=\"flipper_length_mm\", y=\"bill_length_mm\", hue=\"species\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.catplot(data=train[train['income_total']>=],kind='violin',x='credit',y='income_total')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.catplot(data=train, kind=\"violin\", x=\"gender\", y=\"income_total_log\", hue=\"family_type\",split=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 범주 - 연속\nsns.catplot(data=tips, kind=\"swarm\", x=\"day\", y=\"total_bill\", hue=\"smoker\",split=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## torch","metadata":{}},{"cell_type":"code","source":"# import torch\n# from torch.autograd import Variable\n# import torch.nn as nn\n# import torch.nn.functional as F\n# import torch.optim as optim\n# from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train=pd.get_dummies(train,drop_first=True)\n# test=pd.get_dummies(test,drop_first=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train.shape,test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X=train.drop('credit',axis=1)\n# y=train['credit']\n\n# from sklearn.preprocessing import normalize\n\n# X=normalize(X,axis=1)\n# testc=test.copy()\n# testc=normalize(testc,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n\n# X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.15,random_state=71,stratify=y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import random\n\n# def set_seed(seed,mode=None):\n#     torch.manual_seed(seed)\n#     torch.backends.cudnn.deterministic=True\n#     torch.backends.cudnn.benchmark=False\n#     np.random.seed(seed)\n#     random.seed(seed)\n#     if mode=='reproductibility':\n#         torch.cuda.manual_seed(seed)\n#         torch.cuda.manual_seed_all(seed)\n        \n# set_seed(71)\n\n# # def seed_torch(seed=42):\n# #     random.seed(seed)\n# #     os.environ['PYTHONHASHSEED'] = str(seed)\n# #     np.random.seed(seed)\n# #     torch.manual_seed(seed)\n# #     torch.cuda.manual_seed(seed)\n# #     torch.backends.cudnn.deterministic = True\n\n# # seed_torch(seed=seed_value)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class CustomDataset(Dataset):\n#     def __init__(self,X_data,y_data=None,train=True):\n#         self.X_data=X_data\n#         if train:\n#             self.y_data=y_data\n        \n#     def __len__(self):\n#         return len(self.X_data)\n    \n#     def __getitem__(self,index):\n#         if train:\n#             return self.X_data[index],self.y_data[index]\n#         else:\n#             return self.X_data[index]\n    \n# train_dataset=CustomDataset(torch.from_numpy(X_train).float(),torch.from_numpy(y_train.values).long())\n# val_dataset=CustomDataset(torch.from_numpy(X_test).float(),torch.from_numpy(y_test.values).long())\n# # test_dataset=CustomDataset(torch.from_numpy(testc).float())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# target_list = []\n# for _, t in train_dataset:\n#     target_list.append(t)\n    \n# target_list = torch.tensor(target_list)\n# target_list = target_list[torch.randperm(len(target_list))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_class_distribution(obj):\n#     count_dict = {\n#         \"0\": 0,\n#         \"1\": 0,\n#         \"2\": 0\n#     }\n    \n#     for i in obj:\n#         if i == 0: \n#             count_dict['0'] += 1\n#         elif i == 1: \n#             count_dict['1'] += 1\n#         elif i == 2: \n#             count_dict['2'] += 1            \n#         else:\n#             print(\"Check classes.\")\n            \n#     return count_dict\n\n# class_count = [i for i in get_class_distribution(y_train).values()]\n# class_weights = 1./torch.tensor(class_count, dtype=torch.float) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weights_all = class_weights[target_list]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# weighted_sampler = WeightedRandomSampler(\n#     weights=class_weights_all,\n#     num_samples=len(class_weights_all),\n#     replacement=True\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EPOCHS = 10\n# BATCH_SIZE = 16\n# LEARNING_RATE = 0.0007\n# NUM_FEATURES = 47\n# NUM_CLASSES = 3\n\n# device='cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_loader=DataLoader(dataset=train_dataset,\n#                           batch_size=BATCH_SIZE,\n#                           sampler=weighted_sampler\n# )\n# val_loader=DataLoader(dataset=val_dataset,batch_size=1)\n# test_loader=DataLoader(dataset=test_dataset,batch_size=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class MCNet(nn.Module):\n#     def __init__(self,num_feature,num_class):\n#         super(MCNet,self).__init__()\n#         self.layer_1 = nn.Linear(num_feature, 512)\n#         self.layer_2 = nn.Linear(512, 128)\n#         self.layer_3 = nn.Linear(128, 64)\n#         self.layer_out = nn.Linear(64, num_class) \n        \n#         self.relu = nn.ReLU()\n# #         self.dropout = nn.Dropout(p=0.2)\n# #         self.batchnorm1 = nn.BatchNorm1d(512)\n# #         self.batchnorm2 = nn.BatchNorm1d(128)\n# #         self.batchnorm3 = nn.BatchNorm1d(64)\n        \n#     def forward(self, x):\n#         x = self.layer_1(x)\n# #         x = self.batchnorm1(x)\n#         x = self.relu(x)\n        \n#         x = self.layer_2(x)\n# #         x = self.batchnorm2(x)\n#         x = self.relu(x)\n# #         x = self.dropout(x)\n        \n#         x = self.layer_3(x)\n# #         x = self.batchnorm3(x)\n#         x = self.relu(x)\n# #         x = self.dropout(x)\n        \n#         x = self.layer_out(x)\n        \n#         return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = MCNet(num_feature = NUM_FEATURES, num_class=NUM_CLASSES)\n# model.to(device)\n\n# criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n# optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install torchsummary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from torchsummary import summary\n\n# summary(model,(1,47))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def multi_acc(y_pred, y_test):\n#     y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n#     y_pred_tags = torch.max(y_pred_softmax, dim = 1)[1]\n    \n#     correct_pred = (y_pred_tags == y_test).float()\n#     acc = correct_pred.sum() / len(correct_pred)\n    \n#     acc = torch.round(acc * 100)\n    \n#     return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy_stats = {\n#     'train': [],\n#     \"val\": []\n# }\n# loss_stats = {\n#     'train': [],\n#     \"val\": []\n# }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# batch_size=128\n# n_iters=5000\n# num_epochs=n_iters/128\n# num_epochs=int(num_epochs)\n\n# # Traning the Model\n# count=0\n# loss_list=[]\n# iteration_list=[]\n# accuracy_list=[]\n\n# for epoch in range(num_epochs):\n#     for i,(train_x,train_y) in enumerate(train_loader):\n        \n#         # Define variables\n#         train_x=Variable(train_x).to(device)\n#         train_y=Variable(train_y).to(device)\n        \n#         # Clear gradients\n#         optimizer.zero_grad()\n\n#         # Forward propagation\n#         outputs=model(train_x)\n        \n#         # Calculate softmax and cross entropy loss\n#         loss=criterion(outputs,train_y)\n        \n#         # Calculate gradients\n#         loss.backward()\n        \n#         # Update parameters\n#         optimizer.step()\n        \n#         count+=1\n        \n#         # validation\n#         if count%50==0:\n#             # Calculate Accuracy         \n#             correct=0\n#             total=0\n#             # Predict test dataset\n#             for val_x,val_y in val_loader: \n#                 val=Variable(val_x).to(device)\n#                 labels=Variable(val_y).to(device)\n                \n#                 # Forward propagation\n#                 outputs=model(val)\n                \n#                 # Get predictions from the maximum value\n#                 predicted=torch.max(outputs.data,1)[1]\n                \n#                 # Total number of labels\n# #                 total+=len(labels)\n                \n#                 # Total correct predictions\n# #                 correct+=(predicted==labels).sum()\n            \n# #             accuracy=100*correct/float(total)\n            \n#             # store loss and iteration\n#             loss_list.append(loss.data)\n#             iteration_list.append(count)\n# #             accuracy_list.append(accuracy)\n#         if count%500==0:\n#             # Print Loss\n#             print(f'Iteration: {count}  Loss: {loss.data}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"Begin training.\")\n# for e in range(1, EPOCHS+1):\n    \n#     # TRAINING\n#     train_epoch_loss = 0\n#     train_epoch_acc = 0\n#     model.train()\n#     for X_train_batch, y_train_batch in train_loader:\n#         X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n#         optimizer.zero_grad()\n        \n#         y_train_pred = model(X_train_batch)\n        \n#         train_loss = criterion(y_train_pred, y_train_batch)\n#         train_loss = Variable(train_loss, requires_grad = True)\n#         train_acc = multi_acc(y_train_pred, y_train_batch)\n        \n#         train_loss.backward()\n#         optimizer.step()\n        \n#         train_epoch_loss += train_loss.item()\n#         train_epoch_acc += train_acc.item()\n        \n        \n#     # VALIDATION    \n#     with torch.no_grad():\n        \n#         val_epoch_loss = 0\n#         val_epoch_acc = 0\n        \n#         model.eval()\n#         for X_val_batch, y_val_batch in val_loader:\n#             X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n            \n#             y_val_pred = model(X_val_batch)\n                        \n#             val_loss = criterion(y_val_pred, y_val_batch)\n#             val_acc = multi_acc(y_val_pred, y_val_batch)\n            \n#             val_epoch_loss += val_loss.item()\n#             val_epoch_acc += val_acc.item()\n#     loss_stats['train'].append(train_epoch_loss/len(train_loader))\n#     loss_stats['val'].append(val_epoch_loss/len(val_loader))\n#     accuracy_stats['train'].append(train_epoch_acc/len(train_loader))\n#     accuracy_stats['val'].append(val_epoch_acc/len(val_loader))\n                              \n    \n#     print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f} | Train Acc: {train_epoch_acc/len(train_loader):.3f}| Val Acc: {val_epoch_acc/len(val_loader):.3f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with torch.no_grad():\n#     model.eval()\n#     data=torch.from_numpy(testc).float()\n#     res=model(data)\n#     preds=torch.max(res,1)[1]\n#     preds = [a.squeeze().tolist() for a in preds]","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}